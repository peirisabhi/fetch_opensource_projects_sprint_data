{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T02:56:33.338255Z",
     "start_time": "2025-03-26T02:56:32.803351Z"
    }
   },
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from requests.auth import HTTPBasicAuth"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T02:56:34.005303Z",
     "start_time": "2025-03-26T02:56:34.002135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "USERNAME = \"feenionloyed\"\n",
    "PASSWORD = \"2001adp@549\""
   ],
   "id": "ea78aed53fb8271d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T02:56:35.001268Z",
     "start_time": "2025-03-26T02:56:34.995850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Store all data\n",
    "all_boards = []\n",
    "all_sprints = []\n",
    "all_issues = []\n",
    "all_backlog_issues = []"
   ],
   "id": "23e5fea93774e6f2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T02:56:35.788861Z",
     "start_time": "2025-03-26T02:56:35.782733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get all boards\n",
    "def fetch_all_boards():\n",
    "    boards = []\n",
    "    start_at = 0\n",
    "    max_results = 50\n",
    "    is_last = False\n",
    "\n",
    "    while not is_last:\n",
    "        url = f\"https://issues.apache.org/jira/rest/agile/1.0/board?startAt={start_at}&maxResults={max_results}\"\n",
    "        response = requests.get(url, auth=HTTPBasicAuth(USERNAME, PASSWORD))\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching boards: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        boards.extend(data['values'])\n",
    "        is_last = data['isLast']\n",
    "        start_at += max_results\n",
    "        time.sleep(1)  # Respect rate limits\n",
    "\n",
    "    return boards\n"
   ],
   "id": "32dc8c68618712c3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T02:56:36.615945Z",
     "start_time": "2025-03-26T02:56:36.594758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get sprints for a board\n",
    "def fetch_sprints(board_id):\n",
    "    sprints = []\n",
    "    start_at = 0\n",
    "    max_results = 50\n",
    "    is_last = False\n",
    "\n",
    "    while not is_last:\n",
    "        url = f\"https://issues.apache.org/jira/rest/agile/1.0/board/{board_id}/sprint?startAt={start_at}&maxResults={max_results}\"\n",
    "        response = requests.get(url, auth=HTTPBasicAuth(USERNAME, PASSWORD))\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            # Some boards may not have sprints or might be inaccessible\n",
    "            print(f\"Error fetching sprints for board {board_id}: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            sprints.extend(data['values'])\n",
    "            is_last = data.get('isLast', True)\n",
    "            start_at += max_results\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Respect rate limits\n",
    "\n",
    "    return sprints\n"
   ],
   "id": "292e9e2f04b12e56",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T02:56:37.379069Z",
     "start_time": "2025-03-26T02:56:37.373626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get issues for a sprint\n",
    "def fetch_sprint_issues(board_id, sprint_id):\n",
    "    issues = []\n",
    "    start_at = 0\n",
    "    max_results = 50\n",
    "    is_last = False\n",
    "\n",
    "    while not is_last:\n",
    "        url = f\"https://issues.apache.org/jira/rest/agile/1.0/board/{board_id}/sprint/{sprint_id}/issue?startAt={start_at}&maxResults={max_results}\"\n",
    "        response = requests.get(url, auth=HTTPBasicAuth(USERNAME, PASSWORD))\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching issues for board {board_id}, sprint {sprint_id}: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            issues.extend(data['issues'])\n",
    "            is_last = data.get('isLast', True)\n",
    "            start_at += max_results\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Respect rate limits\n",
    "\n",
    "    return issues\n",
    "\n"
   ],
   "id": "f8b11c7bae7f4ac4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T02:56:38.248575Z",
     "start_time": "2025-03-26T02:56:38.241113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get backlog issues\n",
    "def fetch_backlog_issues(board_id):\n",
    "    issues = []\n",
    "    start_at = 0\n",
    "    max_results = 50\n",
    "    is_last = False\n",
    "\n",
    "    while not is_last:\n",
    "        url = f\"https://issues.apache.org/jira/rest/agile/1.0/board/{board_id}/backlog?startAt={start_at}&maxResults={max_results}\"\n",
    "        response = requests.get(url, auth=HTTPBasicAuth(USERNAME, PASSWORD))\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching backlog for board {board_id}: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            issues.extend(data['issues'])\n",
    "            is_last = data.get('isLast', True)\n",
    "            start_at += max_results\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Respect rate limits\n",
    "\n",
    "    return issues\n",
    "\n"
   ],
   "id": "86a76db9c2e0f5aa",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:20:30.587599Z",
     "start_time": "2025-03-26T18:20:20.884138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Fetching all boards...\")\n",
    "boards = fetch_all_boards();\n",
    "print(f\"Found {len(boards)} boards\")\n",
    "# print(boards)"
   ],
   "id": "9086b6711f923460",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all boards...\n",
      "Found 251 boards\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:20:33.459662Z",
     "start_time": "2025-03-26T18:20:33.455966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "boards = boards[:20]\n",
    "# print(boards)"
   ],
   "id": "6b6f5647004966f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:26:50.965206Z",
     "start_time": "2025-03-26T18:26:38.054280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for board in boards:\n",
    "    board_id = board['id']\n",
    "\n",
    "    if(board[\"type\"] == \"scrum\"):\n",
    "        print(f\"Processing board: {board['name']} (ID: {board_id})\")\n",
    "        # Get sprints\n",
    "        sprints = fetch_sprints(board_id)\n",
    "        print(f\"Found {len(sprints)} sprints\")\n",
    "\n",
    "        for sprint in sprints:\n",
    "            sprint['boardId'] = board_id\n",
    "            all_sprints.append(sprint)\n",
    "\n",
    "            # Get issues for this sprint\n",
    "            sprint_id = sprint['id']\n",
    "            issues = fetch_sprint_issues(board_id, sprint_id)\n",
    "            print(f\"Found {len(issues)} issues\")\n",
    "\n",
    "            for issue in issues:\n",
    "                dummy_issue = {}\n",
    "                dummy_issue['boardId'] = board_id\n",
    "                dummy_issue['sprintId'] = sprint_id\n",
    "                dummy_issue['key'] = issue['key']\n",
    "                dummy_issue['priority'] = issue['priority']['name']\n",
    "                dummy_issue['status'] = issue['status']['name']\n",
    "                dummy_issue['status_category'] = issue['status']['statusCategory']['name']\n",
    "                dummy_issue['creator'] = issue['creator']['displayName']\n",
    "                dummy_issue['reporter'] = issue['reporter']['displayName']\n",
    "                dummy_issue['closedSprints'] = issue['closedSprints']['id']\n",
    "                dummy_issue['progress'] = issue['progress']['progress']\n",
    "                dummy_issue['progress_total'] = issue['progress']['total']\n",
    "                dummy_issue['worklog'] = issue['progress']['total']\n",
    "                dummy_issue['issuetype'] = issue['issuetype']['name']\n",
    "                dummy_issue['project'] = issue['project']['name']\n",
    "                dummy_issue['created'] = issue['created']\n",
    "                dummy_issue['updated'] = issue['updated']\n",
    "                dummy_issue['description'] = issue['description']\n",
    "                dummy_issue['summary'] = issue['summary']\n",
    "                dummy_issue['duedate'] = issue['duedate']\n",
    "                print(dummy_issue)\n",
    "                all_issues.append(dummy_issue)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Get backlog issues\n",
    "        backlog = fetch_backlog_issues(board_id)\n",
    "        print(f\"Found {len(backlog)} backlog issues\")\n",
    "        for issue in backlog:\n",
    "            # try:\n",
    "            dummy_backlog_issue = {}\n",
    "            dummy_backlog_issue['boardId'] = board_id\n",
    "            dummy_backlog_issue['isBacklog'] = True\n",
    "\n",
    "\n",
    "            fix_versions = issue[\"fields\"].get(\"fixVersions\", [])\n",
    "            dummy_backlog_issue['fixVersion'] = fix_versions[0][\"name\"] if fix_versions else None\n",
    "            dummy_backlog_issue['priority'] = issue[\"fields\"].get(\"priority\", {}).get(\"name\")\n",
    "\n",
    "            dummy_backlog_issue['assignee'] = issue[\"fields\"].get(\"assignee\") if issue[\"fields\"].get(\"displayName\") else None\n",
    "\n",
    "            dummy_backlog_issue['status'] = issue[\"fields\"].get(\"status\", {}).get(\"name\")\n",
    "            dummy_backlog_issue['status_category'] = issue[\"fields\"].get(\"status\", {}).get(\"status_category\", {}).get(\"name\")\n",
    "            dummy_backlog_issue['creator'] = issue[\"fields\"].get(\"creator\", {}).get(\"displayName\")\n",
    "            dummy_backlog_issue['reporter'] = issue[\"fields\"].get(\"reporter\", {}).get(\"displayName\")\n",
    "            dummy_backlog_issue['issuetype'] = issue[\"fields\"].get(\"issuetype\", {}).get(\"name\")\n",
    "            dummy_backlog_issue['project'] = issue[\"fields\"].get(\"project\", {}).get(\"name\")\n",
    "            dummy_backlog_issue['created'] = issue[\"fields\"].get(\"created\")\n",
    "            dummy_backlog_issue['updated'] = issue[\"fields\"].get(\"updated\")\n",
    "            dummy_backlog_issue['description'] = issue[\"fields\"].get(\"description\")\n",
    "            dummy_backlog_issue['summary'] = issue[\"fields\"].get(\"summary\")\n",
    "\n",
    "            # except Exception as e:\n",
    "            #         print('error on - ' + str(e))\n",
    "            #         continue\n",
    "\n",
    "            all_backlog_issues.append(dummy_backlog_issue)\n",
    "            print(dummy_backlog_issue)\n",
    "            print(\"-------------------\")"
   ],
   "id": "e73bd725dc2289b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing board: 0.6.1 (ID: 365)\n",
      "Found 0 sprints\n",
      "Found 50 backlog issues\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Christofer Dutz', 'reporter': 'Christofer Dutz', 'issuetype': 'New Feature', 'project': 'Apache PLC4X', 'created': '2017-12-19T10:39:23.000+0000', 'updated': '2022-11-09T10:14:11.000+0000', 'description': \"In contrast to the already implemented S7 communication, the Profinet Protocol is a different Protocol with different feature set. In order to implement a Profinet driver it is required to become Member of the Profinet Consortium. This membership is ties with an annual membership fee. The ASF could become a member. The problem with this is however that the ASF doesn't become  member in things that cost the ASF money. I have discussed this issue with the CEO of Profinet Europe and there might be an option for the ASF to become a member and someone else paying the bill. Only members seem to be allowed to advertise with the Profinet Logo and call their products Profinet Compatible.\", 'summary': 'Implement the Profinet Protocol'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Benedikt Ritter', 'reporter': 'Benedikt Ritter', 'issuetype': 'New Feature', 'project': 'Apache PLC4X', 'created': '2017-12-21T15:39:11.000+0000', 'updated': '2022-11-09T10:14:19.000+0000', 'description': 'We want to implement an idiomatic Scala API:\\r\\n\\r\\n- no use of Java std library types\\r\\n- no use of PLC4X Java API types\\r\\n- no use of exceptions, instead of that use of Either\\r\\n- no use of Java futures, instead use of scala future', 'summary': 'Add minimal idiomatic Scala API'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Tim Mitsch', 'reporter': 'Tim Mitsch', 'issuetype': 'Improvement', 'project': 'Apache PLC4X', 'created': '2018-10-06T10:27:34.000+0000', 'updated': '2022-11-09T10:14:13.000+0000', 'description': 'abstract FieldItem class holds non abstract methods that return null values - this might cause strange behavior if some of the subClasses have not overridden those methods.\\r\\n\\r\\nIt may be more intuitive that instead of returning null-values an exception is thrown to make clear that operation fails due to a specific reason\\xa0', 'summary': 'Substitution of returning null-Values by throwing e.g. UnsupportedOperationException'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Blocker', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Julian Feinauer', 'reporter': 'Julian Feinauer', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2019-07-04T06:09:03.000+0000', 'updated': '2022-11-09T10:14:26.000+0000', 'description': 'The following is from the Mailing Thread [1]:\\r\\n\\r\\nIn the last weeks we observed multiple times strange behavior when connecting to Siemens S7 devices.\\r\\nWe have not yet been able to trace it down entirely but I have the assumption that it is an issue with the PooledPlcDriverManager.\\r\\n\\r\\nWhats the issue?\\r\\nWhen doing requests (either via OPM or the “regular” API) we come at a point where all subsequent requests simply fail (and in some cases we were no longer able to send requests to the PLC from other instances, so it looks like the internal server went down).\\r\\n\\r\\nWhats the setup?\\r\\nWhen I remember correctly, all situations where this occurred used the Pool as Basis.\\r\\nWe had it both with OPM and the normal API but NOT with the Scraper.\\r\\n\\r\\nI remember that I spent like a hole day at the Hackathon in Mallorca to get all timeout things to work correctly, as the S7 does not like when you simply cancel your request futures.\\r\\nCurrently there are two “suspects” from my side.\\r\\n\\r\\nFirst, the pool calls the “.connect()” method on a now Connection it establishes but by API convention you also have to do that in your code so it gets called multiple times, which could fuck up stuff.\\r\\nSecond, connection can also time out (but its no future in our API) so in the Scraper I implemented it as Future with timeout (as I’m unsure how everything behaes if the pool starts to initialize a connection but then the “waitTime” times out and it abandons this).\\r\\n\\r\\n[1] https://lists.apache.org/thread.html/328a6780b34b4fd2e3298e9e70340293ebb397b1978a7b631030067e@%3Cdev.plc4x.apache.org%3E', 'summary': '[S7] Communication to S7 PLC dies in some situations'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Minor', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Martin Illecker', 'reporter': 'Martin Illecker', 'issuetype': 'New Feature', 'project': 'Apache PLC4X', 'created': '2019-07-04T12:23:56.000+0000', 'updated': '2022-11-09T10:14:09.000+0000', 'description': 'For some\\xa0reason the response of the PLC seems not to be in\\xa0*LITTLE_ENDIAN*\\xa0format.\\r\\nTherefore, the Byte to Integer conversion in [1] results in a negative number.\\r\\n\\xa0\\r\\nThe issue occurs during the\\xa0*symbolHandle*\\xa0creation process in [2].\\r\\n\\xa0\\r\\nI had a look at the ADS specification [3]\\xa0and the response should be in\\xa0*LITTLE_ENDIAN*\\xa0format, but for some reason it is not.\\r\\n\\xa0\\r\\nAs suggested by [~cdutz] we should create an option to override the endianness in the connection-string.\\r\\n\\xa0\\r\\n[1]\\xa0[https://github.com/apache/plc4x/blob/develop/plc4j/protocols/ads/src/main/java/org/apache/plc4x/java/ads/api/util/UnsignedIntLEByteValue.java#L39]\\r\\n[2]\\xa0[https://github.com/apache/plc4x/blob/develop/plc4j/drivers/ads/src/main/java/org/apache/plc4x/java/ads/connection/AdsAbstractPlcConnection.java#L187]\\r\\n[3]\\xa0[https://infosys.beckhoff.com/index.php?content=../content/1031/tcplclibutilities/html/TcPlcLibUtilities_AddOn_ByteOrder.htm&id=]\\r\\n\\xa0', 'summary': 'ADS Big-Endian Support'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Julian Feinauer', 'reporter': 'Julian Feinauer', 'issuetype': 'New Feature', 'project': 'Apache PLC4X', 'created': '2019-07-25T14:57:16.000+0000', 'updated': '2022-11-09T10:14:18.000+0000', 'description': 'Currently we only support an async API for Requests.\\r\\nBut as creating a new Connection is sometimes even slower it would be good to provide an additional asyc API which returns a Future<PlcConnection> and leaves it to the Client to handle asynchronity.', 'summary': 'Add Async API for new Connections'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Tim Mitsch', 'reporter': 'Tim Mitsch', 'issuetype': 'Test', 'project': 'Apache PLC4X', 'created': '2019-08-04T20:05:55.000+0000', 'updated': '2022-11-09T10:14:15.000+0000', 'description': 'set of connection-tests to\\r\\n * different drivers\\r\\n * different conditions (not available, avab then not avab, firewall issues)\\r\\n * heavy load\\r\\n * ...', 'summary': 'Create a set of connection-tests that potentially could lead to crash or leak'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Sebastian Wiendl', 'reporter': 'Sebastian Wiendl', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2019-11-14T09:17:47.000+0000', 'updated': '2022-11-09T10:14:34.000+0000', 'description': 'My PLC4J demo project at https://github.com/sewiendl/plc4j-demo yields the following output after/during disconnecting from the PLC:{code}all requests took PT2M6.261S\\r\\n\\r\\nNov 14, 2019 10:05:13 AM io.netty.channel.DefaultChannelPipeline onUnhandledInboundException\\r\\nWARNUNG: An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.\\r\\njava.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen\\r\\n\\tat sun.nio.ch.SocketDispatcher.read0(Native Method)\\r\\n\\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)\\r\\n\\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\\r\\n\\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\\r\\n\\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)\\r\\n\\tat io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)\\r\\n\\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1125)\\r\\n\\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:347)\\r\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148)\\r\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:682)\\r\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:617)\\r\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:534)\\r\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\\r\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:906)\\r\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\r\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\r\\n\\tat java.lang.Thread.run(Thread.java:748)\\r\\n\\r\\ndisconnected{code}This bug report relates to my questions on the mailing list. I will also try to hand in a wireshark dump later.', 'summary': 'Exception on S7 disconnect'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Sebastian Wiendl', 'reporter': 'Sebastian Wiendl', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2019-11-14T09:42:53.000+0000', 'updated': '2022-11-09T10:14:31.000+0000', 'description': 'Reading an LREAL in my demo project https://github.com/sewiendl/plc4j-demo/tree/feature/read-lreal causes the following exception:{code}org.apache.plc4x.java.api.exceptions.PlcRuntimeException: Field \\'LReal\\' could not be fetched, response was INVALID_DATATYPE\\r\\n\\tat org.apache.plc4x.java.base.messages.DefaultPlcReadResponse.getFieldInternal(DefaultPlcReadResponse.java:577)\\r\\n\\tat org.apache.plc4x.java.base.messages.DefaultPlcReadResponse.getObject(DefaultPlcReadResponse.java:81)\\r\\n\\tat com.example.plc4jdemo.Main.main(Main.java:39){code}The field was added with {code}                builder.addItem(\"LReal\", \"%DB101.DBX110:LREAL\");{code}', 'summary': 'Reading LREAL from S7 causes PlcRuntimeException'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Christofer Dutz', 'reporter': 'Christofer Dutz', 'issuetype': 'Task', 'project': 'Apache PLC4X', 'created': '2020-01-30T14:14:38.000+0000', 'updated': '2022-11-09T10:14:30.000+0000', 'description': 'Currently the Connection pool only seems to work on the connection string but ignoring the parameters. As the parameters might contain important information (Rack & Slot for S7 or other things like the location of the KNXProj File for KNX or EDE File locations for BACnet) it should treat the entire connection string as pool key.\\xa0', 'summary': 'The connection pool should respect the entire URL'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Christofer Dutz', 'reporter': 'Christofer Dutz', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2020-01-30T15:34:35.000+0000', 'updated': '2022-11-09T10:14:36.000+0000', 'description': 'The build on jenkins randomly seems to fail because of this test:\\r\\n\\r\\nRequestTransactionManagerTest.abortTransactionFromExternally\\r\\nh3. Stacktrace\\r\\n\\r\\njava.lang.AssertionError at org.apache.plc4x.java.spi.optimizer.RequestTransactionManagerTest.abortTransactionFromExternally(RequestTransactionManagerTest.java:153)\\r\\n\\r\\n\\xa0', 'summary': 'The RequestTransactionManagerTest.abortTransactionFromExternally seems to be a little flaky'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': '0.6.1', 'priority': 'Critical', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'César García', 'reporter': 'César García', 'issuetype': 'New Feature', 'project': 'Apache PLC4X', 'created': '2020-03-02T13:32:26.000+0000', 'updated': '2022-11-09T10:19:16.000+0000', 'description': 'The objective of the manual is to show in detail the characteristics of the developed Driver-S7, its potential and points of improvement.\\r\\nIt will be done incapie in the formats of items and in the best practices to optimize the communication process with Siemens S7 PLCs.\\r\\nIt will end with practical examples associated with Continuous (machinery) and Batch (Processes) processes.', 'summary': 'User manual. Driver-S7'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': '0.6.1', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'César García', 'reporter': 'César García', 'issuetype': 'Improvement', 'project': 'Apache PLC4X', 'created': '2020-03-02T14:34:57.000+0000', 'updated': '2022-11-09T10:18:57.000+0000', 'description': '# A request with many Items is divided within the limits allowed by the size of the PDU.\\r\\n # If one of the requested Items exceeds the size of the PDU it is trimmed to the maximum size of the PDU. The existing code tries to split the message, but it fails. This generates an unsafe condition.\\r\\n\\r\\n # Rethink the routine for handling long messages, as an additional layer in Netty.', 'summary': 'Request split / Message Split'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': '0.6.1', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'César García', 'reporter': 'César García', 'issuetype': 'New Feature', 'project': 'Apache PLC4X', 'created': '2020-03-02T14:49:22.000+0000', 'updated': '2022-11-09T10:19:07.000+0000', 'description': '* This version allows the subscription to the following types of events:\\r\\nMODE: Report status of PLC.\\r\\nSYS: Report system events.\\r\\nUSR: Report user events.\\r\\nALM_S: Report ALARM_SQ,ALARM_S,ALARM_SC,ALARM_DQ,ALARM_D.\\r\\nALM_8: Report NOTIFY_8P, ALARM,ALARM_8P, NOTIFY\\r\\n\\r\\n * The ALM_S events are generated by the S7-300. The S7-400 PLCs support ALM_S and ALM_8.\\r\\n * S7-1200 PLCs do not have a message system.\\r\\n * It does not support messages for S7-1500. Next review.', 'summary': 'Subscription to system events.'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': '0.6.1', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'César García', 'reporter': 'César García', 'issuetype': 'New Feature', 'project': 'Apache PLC4X', 'created': '2020-03-02T15:05:46.000+0000', 'updated': '2022-11-09T10:18:59.000+0000', 'description': 'This version implements the subscription to PLC Items under the philosophy \"Don\\'t call me, I\\'ll call you\".\\r\\n The requested items are sent periodically by the PLC to the driver. The driver collects the data and sends it to the client.\\r\\n This solution is ideal for systems that need rapid data updating and low resource consumption.\\r\\n\\r\\nCurrently only S7-300 and S7-400.\\r\\nS7-1500 the next revision.', 'summary': 'Subscription to Items'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': '0.6.1', 'priority': 'Critical', 'assignee': None, 'status': 'In Progress', 'status_category': None, 'creator': 'Julian Feinauer', 'reporter': 'Julian Feinauer', 'issuetype': 'New Feature', 'project': 'Apache PLC4X', 'created': '2020-03-03T20:54:15.000+0000', 'updated': '2022-11-09T10:19:11.000+0000', 'description': 'For an application we need to support Cert based Security in the OPC UA Driver.', 'summary': 'OPC UA Driver should support client certificates'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Turker TUNALI', 'reporter': 'Turker TUNALI', 'issuetype': 'New Feature', 'project': 'Apache PLC4X', 'created': '2020-05-07T23:03:39.000+0000', 'updated': '2022-11-09T10:19:05.000+0000', 'description': 'Apache NiFi integration should allow us to use Expression Language for PLC connection string and PLC resource address string.\\r\\n\\r\\nWe sometimes need to get data from 100 different addresses. Current processors doesn\\'t allow us to create those strings on the fly. So we need to enter them manually or we need utilize NiFi API to create flows automatically.\\xa0\\r\\n\\r\\nIf those parameters allows us to use expression language, we can read a list from csv file or from database and then we can read them in a loop in Apache NiFi.\\xa0\\xa0\\r\\n\\r\\nSo it will be very handy feature to dynamically specify the connection string and the address string.\\r\\n\\r\\nFor starting point PutFile processor can be examined. This processor utilizes expression language for it\\'s \"Directory\" parameter.\\r\\n\\r\\n[https://github.com/apache/nifi/blob/master/nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/java/org/apache/nifi/processors/standard/PutFile.java]\\r\\n\\r\\nNotable Lines:\\r\\n\\r\\nimport org.apache.nifi.expression.ExpressionLanguageScope;\\r\\n\\r\\n...\\r\\n\\r\\nDefine the parameter as expression language supported.\\r\\n\\r\\npublic static final PropertyDescriptor DIRECTORY = new PropertyDescriptor.Builder()\\r\\n .name(\"Directory\")\\r\\n .description(\"The directory to which files should be written. You may use expression language such as /aa/bb/${path}\")\\r\\n .required(true)\\r\\n .addValidator(StandardValidators.NON_EMPTY_VALIDATOR)\\r\\n *.expressionLanguageSupported(ExpressionLanguageScope.FLOWFILE_ATTRIBUTES)*\\r\\n .build();\\r\\n\\r\\n...\\r\\n\\r\\nAnd then script can be evaluated in the onTrigger event like\\r\\n\\r\\ncontext.getProperty(DIRECTORY).evaluateAttributeExpressions(flowFile).getValue()', 'summary': 'Apache NiFi integration should allow Expression Language'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Critical', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Julian Feinauer', 'reporter': 'Julian Feinauer', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2020-05-22T14:37:02.000+0000', 'updated': '2022-11-09T10:19:37.000+0000', 'description': 'When a request is made against a PLC which returns a ResponseCode other than OK the future will never complete and hang forever.', 'summary': 'Request future never returns when ResponseCode not OK for Siemes S7'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Minor', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Niclas Hedhman', 'reporter': 'Niclas Hedhman', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2020-06-25T04:09:00.000+0000', 'updated': '2022-11-09T10:19:18.000+0000', 'description': \"\\r\\nIn the testcase below, there is a permission problem and I think because of that the closing of channel and buffers are out of order/state.\\r\\n\\r\\n\\r\\n[code]\\r\\n11:59:51.855 [pool-1-thread-1] WARN  i.n.c.AbstractChannelHandlerContext - Failed to mark a promise as failure because it has succeeded already: DefaultChannelPromise@63ab80ee(success)\\r\\njava.lang.IllegalStateException: close() must be invoked after the channel is closed.\\r\\n        at io.netty.channel.ChannelOutboundBuffer.close(ChannelOutboundBuffer.java:683)\\r\\n        at io.netty.channel.ChannelOutboundBuffer.close(ChannelOutboundBuffer.java:711)\\r\\n        at io.netty.channel.AbstractChannel$AbstractUnsafe.close(AbstractChannel.java:741)\\r\\n        at io.netty.channel.AbstractChannel$AbstractUnsafe.close(AbstractChannel.java:607)\\r\\n        at io.netty.channel.DefaultChannelPipeline$HeadContext.close(DefaultChannelPipeline.java:1352)\\r\\n        at io.netty.channel.AbstractChannelHandlerContext.invokeClose(AbstractChannelHandlerContext.java:622)\\r\\n        at io.netty.channel.AbstractChannelHandlerContext.close(AbstractChannelHandlerContext.java:606)\\r\\n        at io.netty.channel.AbstractChannelHandlerContext.close(AbstractChannelHandlerContext.java:472)\\r\\n        at io.netty.channel.DefaultChannelPipeline.close(DefaultChannelPipeline.java:957)\\r\\n        at io.netty.channel.AbstractChannel.close(AbstractChannel.java:232)\\r\\n        at io.netty.channel.ChannelFutureListener$2.operationComplete(ChannelFutureListener.java:56)\\r\\n        at io.netty.channel.ChannelFutureListener$2.operationComplete(ChannelFutureListener.java:52)\\r\\n        at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\\r\\n        at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\\r\\n        at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\\r\\n        at io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:183)\\r\\n        at io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)\\r\\n        at io.netty.bootstrap.Bootstrap$3.run(Bootstrap.java:248)\\r\\n        at io.netty.channel.ThreadPerChannelEventLoop.run(ThreadPerChannelEventLoop.java:69)\\r\\n        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\\r\\n        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\r\\n        at java.lang.Thread.run(Thread.java:748)\\r\\n[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.26 s <<< FAILURE! - in org.apache.plc4x.java.utils.rawsockets.netty.RawSocketChannelTest\\r\\n[ERROR] doConnect  Time elapsed: 2.478 s  <<< ERROR!\\r\\norg.pcap4j.core.PcapNativeException: lo: You don't have permission to capture on that device (socket: Operation not permitted)\\r\\n[code]\", 'summary': 'Incorrect shutdown sequence on error'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Christofer Dutz', 'reporter': 'Christofer Dutz', 'issuetype': 'Task', 'project': 'Apache PLC4X', 'created': '2020-07-08T18:04:51.000+0000', 'updated': '2022-11-09T10:19:33.000+0000', 'description': 'The S7 will respond with:\\r\\n * Error Class: 0x83\\r\\n * Error Code: 0x04\\xa0\\r\\n\\r\\nif the user tries a write request and this is not explicitly enabled, we can definitely handle this in a nicer way.\\r\\n\\r\\nTo fix the problem, you need to select the PLC in TIA, go into the Properties dialog, select \"Protection\". You will probably notice there\\'s an Access Level Table, but you need to scroll down (even if it looks as if there is nothing). There check the box in: \"Permit access with PUT/GET communications from remote partner....\"\\r\\n\\r\\nWould be cool if we could give our users a hint on this.', 'summary': '[S7] When trying to write to a S7 device and writing is not explicitly enabled, the PLC will respond with an error code'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Vikram Gopu', 'reporter': 'Vikram Gopu', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2020-07-17T09:59:46.000+0000', 'updated': '2022-11-09T10:19:29.000+0000', 'description': \"I ran twincat simulator on my local host machine with ip 192.168.x.x subnetwork and the similator has the ip address\\xa0172.21.97.81, and then i have used the ads server connection string:\\xa0ads:tcp://localhost/172.21.97.81.1.1:851, which seems not to be connected and i receive the error message as shown in the logs. Can someone point out what the problem is or the bug is ?\\r\\n\\r\\n\\xa0\\r\\n\\r\\nBest Regards\\r\\n\\r\\nVikram Gopu\\xa0\\r\\n\\r\\n\\xa0\\r\\n\\r\\n\\xa0\\r\\n\\r\\n\\xa0\\r\\n\\r\\n[main] INFO org.apache.plc4x.java.PlcDriverManager - Instantiating new PLC Driver Manager with class loader jdk.internal.loader.ClassLoaders$AppClassLoader@2626b418[main] INFO org.apache.plc4x.java.PlcDriverManager - Instantiating new PLC Driver Manager with class loader jdk.internal.loader.ClassLoaders$AppClassLoader@2626b418[main] INFO org.apache.plc4x.java.PlcDriverManager - Registering available drivers...[main] INFO org.apache.plc4x.java.PlcDriverManager - Registering driver for Protocol modbus (Modbus (TCP / Serial))[main] INFO org.apache.plc4x.java.PlcDriverManager - Registering driver for Protocol s7 (Siemens S7 (Basic))[main] INFO org.apache.plc4x.java.PlcDriverManager - Registering driver for Protocol ads (Beckhoff Twincat ADS)[main] INFO org.apache.plc4x.java.scraper.config.triggeredscraper.ScraperConfigurationTriggeredImpl - Assuming job as triggered job because triggerConfig has been set[main] INFO org.apache.plc4x.java.scraper.triggeredscraper.TriggeredScraperImpl - Starting jobs...[main] INFO org.apache.plc4x.java.scraper.triggeredscraper.TriggeredScraperImpl - Task TriggeredScraperTask\\\\{driverManager=org.apache.plc4x.java.utils.connectionpool.PooledPlcDriverManager@4b9e255, jobName='ScheduleJob', connectionAlias='DeviceSource', connectionString='ads:tcp://localhost/172.21.97.81.1.1:851', requestTimeoutMs=1000, executorService=java.util.concurrent.ThreadPoolExecutor@5e57643e[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0], resultHandler=eu.cloudplug.cpe.plc4x.PLC4XScrapper$$Lambda$67/0x0000000800bcac40@133e16fd, triggerHandler=org.apache.plc4x.java.scraper.triggeredscraper.triggerhandler.TriggerHandlerImpl@51b279c9} added to scheduling[triggeredscraper-scheduling-thread-1] WARN org.apache.plc4x.java.scraper.triggeredscraper.TriggeredScraperTask - Exception during scraping of Job ScheduleJob, Connection-Alias DeviceSource: Error-message: null - for stack-trace change logging to DEBUG[nioEventLoopGroup-3-1] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.io.netty.handler.codec.DecoderException: java.lang.IndexOutOfBoundsException at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:98) at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1421) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:697) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:830)Caused by: java.lang.IndexOutOfBoundsException at io.netty.buffer.EmptyByteBuf.readUnsignedIntLE(EmptyByteBuf.java:594) at org.apache.plc4x.java.ads.api.util.UnsignedIntLEByteValue.<init>(UnsignedIntLEByteValue.java:53) at org.apache.plc4x.java.ads.api.commands.types.Result.<init>(Result.java:43) at org.apache.plc4x.java.ads.api.commands.types.Result.of(Result.java:59) at org.apache.plc4x.java.ads.protocol.Ads2PayloadProtocol.handleADSReadWriteCommand(Ads2PayloadProtocol.java:367) at org.apache.plc4x.java.ads.protocol.Ads2PayloadProtocol.decode(Ads2PayloadProtocol.java:135) at org.apache.plc4x.java.ads.protocol.Ads2PayloadProtocol.decode(Ads2PayloadProtocol.java:42) at io.netty.handler.codec.MessageToMessageCodec$2.decode(MessageToMessageCodec.java:81) at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:88) ... 22 more\", 'summary': 'ADS connection issue, Help wanted'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Critical', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Otto Fowler', 'reporter': 'Otto Fowler', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2020-07-25T16:08:36.000+0000', 'updated': '2022-11-09T10:19:34.000+0000', 'description': \"NIFI Flowfiles have 'automatic' attributes, that are there even if they aren't explicitly created by a flow processor.\\r\\n\\r\\nThe PLC4x processors use all the attributes when writing, this is incorrect.  They will be writing the wrong things.\\r\\n\\r\\nThe usual pattern in NIFI is to have the attribute name follow a pattern, that can be read as a prefix, such as :\\r\\n\\r\\nplc4x.address.\\r\\n\\r\\nThen the processor looks for any attributes that start with that, and get the end of the address to use as the field name.\\r\\n\\r\\nAs is I would not think this processor would work in production.\\r\\n\\r\\nExample of this pattern in NIFI: \\r\\nhttps://github.com/apache/nifi/blob/7d20c03f89358a5d5c6db63e631013e1c4be4bc4/nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/java/org/apache/nifi/processors/standard/InvokeHTTP.java#L132\\r\\n\\r\\nThis is slightly different, as the processor above is looking for configured values, where I believe that the plc4x processor just want to write anything with the attribute name value.\\r\\n\\r\\nThis is doubly wrong, as attributes are kept in a map.  There will never be more than one named value.\\r\\n\\r\\nThe proposed fix would be ->\\r\\n\\r\\n- change the reads attribute to use a prefix pattern as  above\\r\\n- only write out those that match through regex / capture\\r\\n\\r\\n\\r\\n\", 'summary': 'NIFI processors should only work with specific attributes'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Stefano Bossi', 'reporter': 'Stefano Bossi', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2020-08-22T10:05:59.000+0000', 'updated': '2022-11-09T10:19:45.000+0000', 'description': \"Dear Chris,\\r\\n\\r\\nI need to report an another bug. \\r\\nAs you know I am trying to read a very complex Data Bloc from a S7-1200, in the future I will try with a 1500. A picture of one of this block in in the attached picture.\\r\\nI found a problem which actually is similar to the String problem I reported in [PLC4X-240|https://issues.apache.org/jira/projects/PLC4X/issues/PLC4X-240], in reading long array of Int. \\r\\nThe software running in the PLC as a component which actually samples an analog value a store the data in long Array of Integer or Real. An example is:\\r\\n\\r\\n{code:java}\\r\\nPLC_CellValue_2[400]='%DB2:928.0:INT[400]'\\r\\n{code}\\r\\n\\r\\nor \\r\\n\\r\\n{code:java}\\r\\nPLC_SpeedNotSafety[400]='%DB2:6542.0:REAL[400]'\\r\\n{code}\\r\\n\\r\\nReading such a long array of values is not possible because the request on the wire is for a too huge payload (probably this is the same problem we had with the string). \\r\\nI have attached the wireshark capture for you.\\r\\n\\r\\nI did the comparison with the 0.6.0 version of the library and the array is correctly read. On the wire I see 3 read of 110 Integer and a last one of 70 which are my 400 Integer. \\r\\nAs for this reading I have captured the wireshark.\\r\\n\\r\\njust to complete the big picture my final target is to read a big DB. In this complex scenario there a similar problem that I think is a data request size problem too. \\r\\nIf I try to read a list of variables like this one: \\r\\n\\r\\n{code:java}\\r\\nPLC_ReportColpoDateLast='%DB2:0.0:DATE_AND_TIME'\\r\\nPLC_@timestamp='%DB2:12.0:DATE_AND_TIME'\\r\\nPLC_ReportColpoDateLastID='%DB2:24.0:DINT'\\r\\nPLC_ReportColpoDateID='%DB2:28.0:DINT'\\r\\nPLC_RichiestaCurva='%DB2:32.0:INT'\\r\\nPLC_TrasferimentoCurva='%DB2:34.0:BOOL'\\r\\nPLC_ArchiveReport='%DB2:36.0:BOOL'\\r\\nPLC_DeleteReport='%DB2:36.1:BOOL'\\r\\nPLC_Report='%DB2:36.2:BOOL'\\r\\nPLC_Enable='%DB2:36.3:BOOL'\\r\\nPLC_SetCelloffset='%DB2:36.4:BOOL'\\r\\nPLC_ResCelloffset='%DB2:36.5:BOOL'\\r\\nPLC_IDX='%DB2:38.0:INT'\\r\\nPLC_KW='%DB2:40.0:BOOL'\\r\\nPLC_TemCen='%DB2:42.0:REAL'\\r\\nPLC_TemBiella='%DB2:46.0:REAL'\\r\\nPLC_TemBroDx='%DB2:50.0:REAL'\\r\\nPLC_TemBroSx='%DB2:54.0:REAL'\\r\\nPLC_PreCen='%DB2:58.0:REAL'\\r\\nPLC_PreFreno='%DB2:62.0:REAL'\\r\\nPLC_PreCil='%DB2:66.0:REAL'\\r\\nPLC_EncPosAct='%DB2:70.0:REAL'\\r\\nPLC_EncPosActSafety='%DB2:74.0:REAL'\\r\\nPLC_EncSpeedSafety='%DB2:78.0:REAL'\\r\\nPLC_EncSpeed='%DB2:82.0:REAL'\\r\\nPLC_CellValue[5]='%DB2:86.0:INT[5]'\\r\\nPLC_CellValueOffset[5]='%DB2:96.0:INT[5]'\\r\\nPLC_N_TotPezzi='%DB2:106.0:DINT'\\r\\nPLC_N_TotColpi='%DB2:110.0:DINT'\\r\\nPLC_KW_Max='%DB2:114.0:INT'\\r\\nPLC_CellValueMax[5]='%DB2:116.0:INT[5]'\\r\\nPLC_CellValue_1[2]='%DB2:126.0:INT[2]'\\r\\nPLC_CellValue_2[400]='%DB2:928.0:INT[100]'\\r\\n{code}\\r\\n\\r\\nThe error I find on the wire is the same. Pleas note that in the list of variables there are no array of 400 samples but, I suppose, that the sum of all the request fire the same bug about the request of more than 240 byte length. \\r\\nI have attached a wireshark capture of this scenario too. \\r\\n\\r\\nHope this analysis could help to find an universal solution for this problem. \\r\\n\\r\\nRegards,\\r\\nStefano \\r\\n\\r\\nP.S. As usual I am using the HellpPlc4x code and the latest compiled version of the 0.8.0 library. \", 'summary': 'Reading long Int Array'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Turker TUNALI', 'reporter': 'Turker TUNALI', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2020-08-28T10:09:56.000+0000', 'updated': '2022-11-09T10:19:44.000+0000', 'description': 'My Plc4xSourceProcessor\\'s, PLC connection String is \"modbus:tcp://[10.0.2.238:502?slave=1|http://10.0.2.238:502/?slave=1]\" and PLC resource address String is \"test1=holding-register:1\"\\r\\n \\xa0\\r\\n I can get the values for 5-10 times with 5 second intervals but then I get below exception. I can read the values with the Modbus Poll application, so most probably the PLC4X side has a problem.\\r\\n \\xa0\\r\\n I may also get some other exceptions when starting the processor, which are also below.\\r\\n \\xa0\\r\\nPS: Wireshark trace is attached. I\\'ve read 16 times then I get the exception.\\r\\n \\xa0\\r\\n * \\r\\n -- This is the exception which I get after some successful read operations.*\\r\\n 2020-08-27 13:19:06,091 WARN [nioEventLoopGroup-8-1] io.netty.channel.DefaultChannelPipeline An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.\\r\\n java.io.IOException: An existing connection was forcibly closed by the remote host\\r\\n at sun.nio.ch.SocketDispatcher.read0(Native Method)\\r\\n at sun.nio.ch.SocketDispatcher.read(Unknown Source)\\r\\n at sun.nio.ch.IOUtil.readIntoNativeBuffer(Unknown Source)\\r\\n at sun.nio.ch.IOUtil.read(Unknown Source)\\r\\n at sun.nio.ch.SocketChannelImpl.read(Unknown Source)\\r\\n at io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)\\r\\n at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133)\\r\\n at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\\r\\n at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148)\\r\\n at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\\r\\n at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\\r\\n at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\\r\\n at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\r\\n at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\\r\\n at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\r\\n at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\r\\n at java.lang.Thread.run(Unknown Source)\\r\\n \\xa0\\r\\n \\xa0\\r\\n -- This is the exception which I get sometimes when starting up the processor.*\\r\\n 2020-08-27 13:17:16,813 WARN [Timer-Driven Process Thread-11] o.a.n.controller.tasks.ConnectableTask Administratively Yielding Plc4xSourceProcessor[id=2f12f5b4-0174-1000-724e-a53ba0fc1652] due to uncaught Exception: java.lang.NullPointerException\\r\\n java.lang.NullPointerException: null\\r\\n at org.apache.plc4x.nifi.Plc4xSourceProcessor.onTrigger(Plc4xSourceProcessor.java:50)\\r\\n at org.apache.nifi.processor.AbstractProcessor.onTrigger(AbstractProcessor.java:27)\\r\\n at org.apache.nifi.controller.StandardProcessorNode.onTrigger(StandardProcessorNode.java:1174)\\r\\n at org.apache.nifi.controller.tasks.ConnectableTask.invoke(ConnectableTask.java:213)\\r\\n at org.apache.nifi.controller.scheduling.TimerDrivenSchedulingAgent$1.run(TimerDrivenSchedulingAgent.java:117)\\r\\n at org.apache.nifi.engine.FlowEngine$2.run(FlowEngine.java:110)\\r\\n at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\\r\\n at java.util.concurrent.FutureTask.runAndReset(Unknown Source)\\r\\n at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source)\\r\\n at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)\\r\\n at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\r\\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\r\\n at java.lang.Thread.run(Unknown Source)', 'summary': '[Modbus] Apache NiFi processor throws java.io.IOException after a while'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Minor', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Sebastian Voss', 'reporter': 'Sebastian Voss', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2020-09-09T18:49:05.000+0000', 'updated': '2022-11-09T10:19:39.000+0000', 'description': 'I’m facing some issue when I try to quit my application. It seems connection.close() is not stopping all pending threads. Is this a known issue or am I doing something wrong?\\r\\n\\r\\n\\r\\n{code:java}\\r\\nString url = \"s7://...\";\\r\\nPlcDriverManager manager = new PlcDriverManager();\\r\\nPlcConnection connection = manager.getConnection(url);\\r\\nconnection.close();\\r\\nSystem.out.println(\"closed”); \\xa0// gets printed but hangs afterwards{code}\\r\\nI created a thread dump which is attached.', 'summary': 'Pending threads after connection.close'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Christofer Dutz', 'reporter': 'Christofer Dutz', 'issuetype': 'Task', 'project': 'Apache PLC4X', 'created': '2020-10-29T21:34:01.000+0000', 'updated': '2022-11-09T10:19:41.000+0000', 'description': 'Currently the code generation still uses the Supplier-Based reading of floating-point values. This code seems to have issues with the value \"0.0\", which is displayed as an extremely small floating-point value.\\r\\n\\r\\nWhen switching to ReadBuffer readFloat and readDouble implementation they seem to be causing errors.', 'summary': 'Handling of FLOAT and DOUBLE has issues with generated code'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Trivial', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Ben Hutcheson', 'reporter': 'Ben Hutcheson', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2020-11-18T11:22:58.000+0000', 'updated': '2022-11-09T10:19:48.000+0000', 'description': \"Looking at the statistics debug log, the statistics occasionally report failed requests but on the next cycle may show as all have completed.\\r\\n\\r\\n4 (3 success, 25.0 % failed, 0.0 % too slow)\\r\\n\\r\\nThen on the next reporting cycle may show.\\r\\n\\r\\n7 (7 success, 0.0 % failed, 0.0 % too slow)\\r\\n\\r\\nI'm assuming that it is showing requests that haven't completed as failed requests when it shouldn't count incomplete ones.\\r\\n\\r\\n\\xa0\\r\\n\\r\\nplc4j/tools/scraper/src/main/java/org/apache/plc4x/java/scraper/ScraperImpl.java - Line 146.\", 'summary': 'Scraper Statistics show Incomplete Requests as Failed'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Christofer Dutz', 'reporter': 'Christofer Dutz', 'issuetype': 'Task', 'project': 'Apache PLC4X', 'created': '2021-01-09T18:41:18.000+0000', 'updated': '2022-11-09T10:19:51.000+0000', 'description': 'It would be cool if we could implement the Fatek protocol.\\r\\n\\r\\nProtocol Spec:\\r\\n\\r\\n[http://fatek.com.tr/downloads/yuklemeler/Manual_2_Appendix1.pdf]\\r\\n\\r\\nOpen-Source Project (Apache 2.0 Licensed)\\r\\n\\r\\n[https://github.com/s4u/jfatek]\\r\\n\\r\\n[https://www.simplify4u.org/jfatek/]\\r\\n\\r\\n\\xa0', 'summary': 'Implement the Fatek protocol'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Christofer Dutz', 'reporter': 'Christofer Dutz', 'issuetype': 'Task', 'project': 'Apache PLC4X', 'created': '2021-01-11T10:21:43.000+0000', 'updated': '2022-11-09T10:19:57.000+0000', 'description': \"It seems I skipped porting the code to gracefully close a connection in the transition from 0.6 to 0.7. This is a great low-hanging fruit, so I'll leave this here for someone to picup.\\r\\n\\r\\nWe already generally have the parts in place, however they are not quite correct:\\r\\n\\r\\n[https://github.com/apache/plc4x/blob/develop/protocols/s7/src/main/resources/protocols/s7/s7.mspec]\\r\\n\\r\\nDefines a type:\\xa0COTPPacketDisconnectRequest, however the third parameter is not class, but disconnectReason. \\r\\n\\r\\nThe old TPDU is defined here:\\r\\n[https://github.com/apache/plc4x/blob/rel/0.6/plc4j/protocols/iso-tp/src/main/java/org/apache/plc4x/java/isotp/protocol/model/tpdus/DisconnectRequestTpdu.java]\\r\\n\\r\\nThis should be an enum type. The constant names and values can be taken from here: \\r\\n[https://github.com/apache/plc4x/blob/rel/0.6/plc4j/protocols/iso-tp/src/main/java/org/apache/plc4x/java/isotp/protocol/model/types/DisconnectReason.java]\\r\\n\\r\\nAs soon as these changes are in place and the code has been generated, the logic for closing can be implemented by being inspired by the old drivers code:\\r\\n[https://github.com/apache/plc4x/blob/rel/0.6/plc4j/drivers/s7/src/main/java/org/apache/plc4x/java/s7/connection/S7PlcConnection.java]\", 'summary': '[S7] Implement connection closing for S7 protocol'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'liuqiang', 'reporter': 'liuqiang', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-01-21T02:30:24.000+0000', 'updated': '2022-11-09T10:19:54.000+0000', 'description': \"[main] INFO org.apache.plc4x.java.PlcDriverManager - Instantiating new PLC Driver Manager with class loader sun.misc.Launcher$AppClassLoader@14dad5dc\\r\\n[main] INFO org.apache.plc4x.java.PlcDriverManager - Registering available drivers...\\r\\n[main] INFO org.apache.plc4x.java.PlcDriverManager - Registering driver for Protocol s7 (Siemens S7 (Basic))\\r\\n[main] INFO org.apache.plc4x.java.transport.tcp.TcpChannelFactory - Configuring Bootstrap with Configuration\\\\{local-rack=1, local-slot=1, remote-rack=0, remot-slot=3, pduSize=1024, maxAmqCaller=8, maxAmqCallee=8, controllerType='null'}\\r\\n[nioEventLoopGroup-2-1] INFO org.apache.plc4x.java.s7.readwrite.protocol.S7ProtocolLogic - S7 Driver running in ACTIVE mode.\", 'summary': 'WIthout “S7 Driver running in ACTIVE mode.” Nothing response'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'liuqiang', 'reporter': 'liuqiang', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-01-23T06:16:51.000+0000', 'updated': '2022-11-09T10:20:05.000+0000', 'description': 'When use OPM of plc4j，sometimes meet error “org.apache.plc4x.java.opm.OPMException: Timeout during fetching values”。maybe one item was got more fetche request（in different threads）at a time。', 'summary': 'org.apache.plc4x.java.opm.OPMException: Timeout during fetching values'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Brian Burns', 'reporter': 'Brian Burns', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-02-21T13:29:24.000+0000', 'updated': '2022-11-09T10:20:22.000+0000', 'description': \"Hello, I'm trying to install PLC4X with Docker and get a couple of errors -\\xa0\\r\\n\\r\\n$ docker build -t plc4x .\\r\\n...\\r\\nStep 21/39 : COPY . /ws/\\r\\n ---> e86e4d250f0c\\r\\nStep 22/39 : WORKDIR /ws\\r\\n ---> Running in b2d9525763e0\\r\\nRemoving intermediate container b2d9525763e0\\r\\n ---> 2d714dffc27b\\r\\nStep 23/39 : RUN ./mvnw -P with-boost,with-c,with-cpp,with-dotnet,with-go,with-logstash,with-opcua-werver,with-proxies,with-python,with-logstash,with-sandbox com.offbytwo.maven.plugins:maven-dependency-plugin:3.1.1.MDEP568:go-offline -DexcludeGroupIds=org.apache.plc4x,org.apache.plc4x.examples,org.apache.plc4x.sandbox\\r\\n ---> Running in 97e09acbcdfd\\r\\n/bin/sh: 1: ./mvnw: Permission denied\\r\\nThe command '/bin/sh -c ./mvnw -P with-boost,with-c,with-cpp,with-dotnet,with-go,with-logstash,with-opcua-werver,with-proxies,with-python,with-logstash,with-sandbox com.offbytwo.maven.plugins:maven-dependency-plugin:3.1.1.MDEP568:go-offline -DexcludeGroupIds=org.apache.plc4x,org.apache.plc4x.examples,org.apache.plc4x.sandbox' returned a non-zero code: 126\\r\\n\\r\\nSo I added a step to the Dockerfile line 72\\r\\n\\xa0\\r\\nRUN chmod +x ./mvnw\\r\\nThat got it past that error, then got another one -\\r\\n\\r\\nNon-resolvable parent POM for org.apache.plc4x.sandbox:plc4cpp:[unknown-version]: Could not find artifact org.apache.plc4x.sandbox:plc4x-sandbox:pom:0.8.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 24, column 11\\r\\n\\r\\nI'm not familiar with maven so am not sure how to get past it...\\r\\n\\r\\nThank you for any help -\\xa0\\r\\n\\r\\n\\xa0\", 'summary': 'Docker build not working - mvnw permission denied etc'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'In Progress', 'status_category': None, 'creator': 'Adrian Lazar', 'reporter': 'Adrian Lazar', 'issuetype': 'Improvement', 'project': 'Apache PLC4X', 'created': '2021-03-15T15:36:16.000+0000', 'updated': '2022-11-09T10:20:16.000+0000', 'description': 'I found some work done towards implementing the [profinet-DCP|https://profinetuniversity.com/naming-addressing/profinet-dcp/]\\xa0protocol on the\\xa0 [feature/profinet branch|[https://github.com/apache/plc4x/tree/feature/profinet]].\\r\\n\\r\\nI see that the profinet protocol is not on the develop branch anymore, was it replaced with the s7 protocol?\\r\\n\\r\\nIf someone would have the time to guide me, I could work on it to update, test, and bring this discovery feature to PLC4X.', 'summary': 'Add profinet-DCP'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Christofer Dutz', 'reporter': 'Christofer Dutz', 'issuetype': 'Task', 'project': 'Apache PLC4X', 'created': '2021-03-23T10:25:47.000+0000', 'updated': '2022-11-09T10:20:19.000+0000', 'description': 'I had to disable the ReadBuffer test for reading Strings in PLC4C as this was causing build errors on Windows', 'summary': '[PLC4C] Fix ReadBuffer test for Strings'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Rogier Cobben', 'reporter': 'Rogier Cobben', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-03-30T13:50:24.000+0000', 'updated': '2022-11-09T10:20:09.000+0000', 'description': 'The Mock driver is usefull for developing applications without available hardware. Reads from Mock devices work, but writes seem broken.\\r\\n\\r\\nScenario:\\r\\n\\r\\nimplement *MockDevice* interface and the *write* method. See that this device is set in the connection using the *MockConnection*.*setDevice* method.\\r\\n\\r\\nWhen a WriteRequest is issued on the connection, the *MockDevice*.*write* method is indeed called, but the *value* parameter is always _*null*_. The given value to write is not passed through.\\r\\n\\r\\nDebugging and looking into the sourcecode shows following:\\r\\n\\r\\n*MockConnection.write*\\xa0uses following statement to retrieve the value:\\r\\n\\r\\n\\xa0\\r\\n{code:java}\\r\\n// MockConnection:147\\r\\n((MockField) writeRequest.getField(name)).getPlcValue()\\r\\n{code}\\r\\nThis retrieves the *plcValue* member from the MockField object. This\\xa0 *plcValue* is written to by constructor *MockField(String address, MockPlcValue plcValue)* only. However, I did not find any call to this constructor.\\r\\n\\r\\n\\xa0\\r\\n\\r\\nNote:\\r\\n\\r\\nOther Drivers seem to use\\r\\n{code:java}\\r\\nwriteRequest.getPlcValue(fieldName) \\r\\n{code}\\r\\ninstead and do not have a value member in the Field class. (e.g. Simulator, Modbus )', 'summary': 'Mock driver always writes null value'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Dmitry Shagiakhmetov', 'reporter': 'Dmitry Shagiakhmetov', 'issuetype': 'Wish', 'project': 'Apache PLC4X', 'created': '2021-04-09T08:22:24.000+0000', 'updated': '2022-11-09T10:20:11.000+0000', 'description': 'I want to get data from plc with period = 1 ms. I try it using consumer:\\r\\n{code:java}\\r\\nval modbusConsumerEndpoint = Plc4XEndpoint(modbusEndpointUri, plcComponent).apply {\\r\\n tags = mapOf(fieldName to \"input-register:$plcInputPort\")\\r\\n period = 1\\r\\n}\\r\\n\\r\\nfrom(modbusConsumerEndpoint)\\r\\n                .process {\\r\\n                    it.message.body = (it.message.body as Map<String, Any>)[ModbusEndpointParams.fieldName]\\r\\n                    it.setMainBody(ctx)\\r\\n                }\\r\\n                .marshal().json()\\r\\n                .setHeader(KafkaConstants.KEY, constant(\"\"))\\r\\n                .to(mainKafkaEndpoint){code}\\r\\nBut I have only one loop. {color:#000000}*Plc4XConsumer* may work with trigger, but I couldn\\'t find any examples for that.\\r\\n{color}\\r\\n\\r\\n{color:#000000}Either I try to use camel chain with timer:{color}\\r\\n{code:java}\\r\\nfrom(\"timer:foo?period=1\")\\r\\n .process {\\r\\n it.message.body =\\r\\n mapOf(fieldName to \"input-register:$plcInputPort\")\\r\\n }\\r\\n .to(\"plc4x:modbus://uri\"){code}\\r\\nBut *Plc4XProducer* works only for writting. I solve this problem by creating own Endpoint with custom Producer includes ReadRequestBuilder extend\\xa0 Plc4XEndpoint and Plc4XProducer. It looks not like production decision.\\r\\n\\r\\nWhat is a right way to do this task?', 'summary': 'Reading data from PLC via Modbus with Camel using timer'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Purushotham YB', 'reporter': 'Purushotham YB', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-06-03T13:58:12.000+0000', 'updated': '2022-11-09T10:20:45.000+0000', 'description': 'Hi,\\r\\n\\r\\n\\xa0 \\xa0I am using plc4x for one of our projects to develop a module which uses plc4x to establish communication with a sensor device using modbus RTU/Serial communication.\\r\\n\\r\\nI am able to communicate with the device with TCP transport but not able to do with RTU/Serial communication.\\r\\n\\r\\nI tried to look for some example/documentation on how to use RTU/Serial communication in plc4x but could not find any information in the website or the web.\\r\\n\\r\\nAlso looking at the documentation at [https://plc4x.apache.org/users/protocols/modbus.html]\\xa0it is not clear whether modbus supports serial transport as it only lists tcp and udp.\\r\\n\\r\\n\\xa0\\r\\n|Compatible Transports:| * {{tcp}}\\xa0(Default Port: 502)\\r\\n * {{udp}}\\xa0(Default Port: 502)|\\r\\n\\r\\nKindly request to clarify if serial is supported in modbus? if so could you please point me to an example/documentation which shall be used to understand how to use it for serial communication.\\r\\n\\r\\nAnyways I tried to use the library/driver plc4j-transport-serial\\xa0 [https://plc4x.apache.org/users/transports/serial.html]\\r\\n\\r\\nto communicate with device in which the connection is established but it fails to read the data with following WANING.\\r\\n\\r\\n\\xa0\\r\\n\\r\\n{{2021-06-03-18:13:51.814 [nioEventLoopGroup-2-1] WARN\\xa0 io.netty.channel.nio.NioEventLoop - Selector.select() returned prematurely 512 times in a row; rebuilding Selector org.apache.plc4x.java.transport.serial.SerialPollingSelector@28ecdc0d.}}\\r\\n{{2021-06-03-18:13:59.630 [nioEventLoopGroup-2-1] WARN\\xa0 io.netty.channel.nio.NioEventLoop - Selector.select() returned prematurely 512 times in a row; rebuilding Selector org.apache.plc4x.java.transport.serial.SerialPollingSelector@11c9a1fa.}}\\r\\n\\r\\n{{My Sample code is as follows.}}\\r\\n\\r\\n\\xa0\\r\\n\\r\\n{{ private void plcRtuReader() {}}\\r\\n{{ // unit-identifier=1&}}\\r\\n{{ String connectionString =}}\\r\\n{{ \"modbus:serial://COM5?unit-identifier=1&baudRate=19200&stopBits=\" + SerialPort.ONE_STOP_BIT}}\\r\\n{{ + \"&parityBits=\"}}\\r\\n{{ + SerialPort.NO_PARITY + \"&dataBits=8\";}}\\r\\n{{ System.out.println(\"URL:\" + connectionString);}}\\r\\n{{ try (PlcConnection plcConnection = new PlcDriverManager().getConnection(connectionString)) {}}\\r\\n\\r\\n{{ if (!plcConnection.getMetadata().canRead()) {}}\\r\\n{{ System.out.println(\"This connection doesn\\'t support reading.\");}}\\r\\n{{ return;}}\\r\\n{{ }}}\\r\\n\\r\\n{{ PlcReadRequest.Builder builder = plcConnection.readRequestBuilder();}}\\r\\n{{ builder.addItem(\"value-2\", \"input-register:1[2]\");}}\\r\\n{{PlcReadRequest readRequest = builder.build();}}\\r\\n\\r\\n{{// CompletableFuture<? extends PlcReadResponse> asyncResponse = readRequest.execute();}}\\r\\n{{ PlcReadResponse response = readRequest.execute().get();}}\\r\\n{{ for (String fieldName : response.getFieldNames()) {}}\\r\\n{{ if (response.getResponseCode(fieldName) == PlcResponseCode.OK) {}}\\r\\n{{ int numValues = response.getNumberOfValues(fieldName);}}\\r\\n{{ // If it\\'s just one element, output just one single line.}}\\r\\n{{ if (numValues == 1) {}}\\r\\n{{ System.out.println(\"Value[\" + fieldName + \"]: \" + response.getObject(fieldName));}}\\r\\n{{ }}}\\r\\n{{ // If it\\'s more than one element, output each in a single row.}}\\r\\n{{ else {}}\\r\\n{{ System.out.println(\"Value[\" + fieldName + \"]:\");}}\\r\\n{{ for (int i = 0; i < numValues; i++) {}}\\r\\n{{ System.out.println(\" - \" + response.getObject(fieldName, i));}}\\r\\n{{ }}}\\r\\n{{ }}}\\r\\n{{ }}}\\r\\n{{ // Something went wrong, to output an error message instead.}}\\r\\n{{ else {}}\\r\\n{{ System.out.println(}}\\r\\n{{ \"Error[\" + fieldName + \"]: \" + response.getResponseCode(fieldName).name());}}\\r\\n{{ }}}\\r\\n{{ }}}\\r\\n\\r\\n{{ System.exit(0);}}\\r\\n{{ } catch (PlcConnectionException e) {}}\\r\\n{{ e.printStackTrace();}}\\r\\n{{ } catch (Exception e) {}}\\r\\n{{ e.printStackTrace();}}\\r\\n{{ }}}\\r\\n{{ }}}\\r\\n\\r\\nThanks a lot for your help.\\r\\n\\r\\nRegards,\\r\\nPurushotham', 'summary': 'Plc4x support for RTU/Serial communication for modbus protocol.'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Torsten', 'reporter': 'Torsten', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-07-02T08:03:14.000+0000', 'updated': '2022-11-09T10:20:51.000+0000', 'description': 'the attached screenshot shows an OPCUA response code as it is received by the underlying milo client, but the OPCUA driver reports only NOT_FOUND,\\r\\nit would be great, if the underlying details are included in the OPCUA response code; the real cause for a problem is important for problem solution', 'summary': 'OPCUA response code and details are not reported in PLC response code'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Christofer Dutz', 'reporter': 'Christofer Dutz', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-07-08T09:17:52.000+0000', 'updated': '2022-11-09T10:20:27.000+0000', 'description': 'Just a test-issue to check how the \"Language\" selection works.', 'summary': 'Test-Issue for language selection'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Christofer Dutz', 'reporter': 'Christofer Dutz', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-07-28T09:28:31.000+0000', 'updated': '2022-11-09T10:20:33.000+0000', 'description': 'When generating code for count-arrays a\\xa0numberOfBytes variable is declared automatically. As soon as one type contains two count-arrays the second one has a compile error: \"no new variables on left side of :=\" ... should be a simple fix ...\\xa0', 'summary': '[Go] [CodeGeneration] When generating code for two arrays within one type, the output is invalid'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'lihs', 'reporter': 'lihs', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-07-29T05:20:26.000+0000', 'updated': '2022-11-09T10:20:37.000+0000', 'description': 'Creating a write request for real type\\xa0 such as\\xa0\\r\\n\\r\\nvia addItem(\"value-5\", \"%DB1.DBD16:REAL\", 12.1)\\r\\n\\r\\n\\xa0\\r\\n\\r\\nwhen execute the request, error as below:\\xa0\\r\\n\\r\\n\\'ERROR org.apache.plc4x.java.s7.readwrite.types.DataTransportErrorCode - No DataTransportErrorCode for value 7\\'', 'summary': 'plc4x s7 driver java for real type wirte error'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'In Progress', 'status_category': None, 'creator': 'Maida Baralić', 'reporter': 'Maida Baralić', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-08-05T09:41:11.000+0000', 'updated': '2022-11-09T10:20:40.000+0000', 'description': \"I'm having some issues with writing a byte array using S7 driver to the S7-1200.\\r\\n\\r\\nI'm a bit new to the communication with PLCs, so I'm not sure if I'm doing something wrong.\\r\\n\\r\\nI've followed the example of how to write a byte array to the connection, however I'm getting an INTERNAL_ERROR as a response code from the library and in the Wireshark the status from the PLC is inconsistent data type.\\xa0\\r\\n\\r\\nI've attached both the sample project that's not working for me, my definition of byte array on the PLC and also\\xa0 Wireshark capture.\\xa0\\r\\n\\r\\nIt's possible that I'm not doing something right, however I can't figure it out.\\xa0\\r\\n\\r\\nWriting of most of the single values is working, however the array is the problem. And as far as I understood from the code and documentation arrays should be supported.\\xa0\\r\\n\\r\\nAny help would be appreciated.\", 'summary': '[S7] Writing byte array not working'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'lihs', 'reporter': 'lihs', 'issuetype': 'New Feature', 'project': 'Apache PLC4X', 'created': '2021-08-13T02:14:43.000+0000', 'updated': '2022-11-09T10:20:56.000+0000', 'description': 'Dose the plc4x for go support S7 PLC ?\\r\\n\\r\\nNo register s7 driver function ?\\r\\n\\r\\nThanks!\\xa0', 'summary': 'plc4x for go language support s7 PLC?'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'karthik', 'reporter': 'karthik', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-09-04T13:36:17.000+0000', 'updated': '2022-11-09T10:21:13.000+0000', 'description': 'Hi team -\\r\\n\\r\\ni had followed the [https://github.com/apache/plc4x/tree/rel/0.8/plc4j/examples/hello-world-plc4x-subscription] and facing the below error .\\r\\n\\r\\nversion : plc4j-driver-opcua-0.8.0\\r\\n\\r\\njava : 1.8\\r\\n\\r\\nException in thread \"main\" java.util.concurrent.ExecutionException: java.lang.ClassCastException: org.apache.plc4x.java.spi.model.DefaultPlcSubscriptionField cannot be cast to org.apache.plc4x.java.opcua.protocol.OpcuaField\\r\\n\\r\\n\\xa0\\r\\n\\r\\nplease kindly provide your suggestions.', 'summary': 'java.lang.ClassCastException: DefaultPlcSubscriptionField cannot be cast to class OpcuaField'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Jeremy Theocharis', 'reporter': 'Jeremy Theocharis', 'issuetype': 'Wish', 'project': 'Apache PLC4X', 'created': '2021-09-22T17:52:08.000+0000', 'updated': '2022-11-09T10:21:26.000+0000', 'description': 'Hi everyone!\\r\\n\\r\\nMy name is Jeremy and I am the lead developer of the United Manufacturing Hub.\\r\\n\\r\\nThe United Manufacturing Hub is an open-source industrial IoT and manufacturing application platform enabling users to connect, store, and access all relevant data sources in industrial manufacturing sites and build user-centric dashboards and applications.\\r\\n\\r\\nIt would be really great if we could make a connection between PLC4X and the United Manufacturing Hub, e.g., in form of a configurable microservice that automatically takes data from various PLCs and pushes them into a MQTT broker. The United Manufacturing Hub (incl. Node-RED, Grafana, timescaleDB, VerneMQ) then provides the infrastructure and data models to contextualize the data and enable use-cases like OEE, Performance Management, Machine to Machine Communication, Digital Shadow, etc. Furthermore, the United Manufacturing Hub allows extracting data from other data sources as well like sensors, barcodereader or cameras.\\r\\n\\r\\nFurther information can be found here:\\r\\n * [https://docs.umh.app/docs/]\\r\\n * [https://github.com/united-manufacturing-hub/united-manufacturing-hub]\\r\\n\\r\\nLooking forward to discussing how we can combine both open-source projects the most effective way.\\r\\n\\r\\nRegards,\\r\\nJeremy', 'summary': 'Connection to United Manufacturing Hub'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'LiuYuHang', 'reporter': 'LiuYuHang', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-10-19T03:01:59.000+0000', 'updated': '2022-11-09T10:21:00.000+0000', 'description': 'Exception thrown when writing string type: java.nio.bufferoverflow exception', 'summary': 'S7 Cannot write string'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Łukasz Dywicki', 'reporter': 'Łukasz Dywicki', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-10-21T21:11:57.000+0000', 'updated': '2022-11-09T10:21:04.000+0000', 'description': \"Issue reported on mailing lists:\\r\\n\\r\\n{code}\\r\\n[nioEventLoopGroup-2-1] WARN io.netty.channel.DefaultChannelPipeline - An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.\\r\\nio.netty.handler.codec.DecoderException: java.lang.ClassCastException: class org.apache.plc4x.java.opcua.readwrite.ServiceFault cannot be cast to class org.apache.plc4x.java.opcua.readwrite.ReadResponse (org.apache.plc4x.java.opcua.readwrite.ServiceFault and org.apache.plc4x.java.opcua.readwrite.ReadResponse are in unnamed module of loader 'app')\\r\\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:98)\\r\\n        at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111)\\r\\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\r\\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\r\\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\r\\n        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\\r\\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\\r\\n        at io.netty.handler.codec.ByteToMessageCodec.channelRead(ByteToMessageCodec.java:103)\\r\\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\r\\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\r\\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\r\\n        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\r\\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\r\\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\r\\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\r\\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\r\\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\r\\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\r\\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\r\\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\r\\n        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\r\\n        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\r\\n        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\r\\n        at java.base/java.lang.Thread.run(Thread.java:834)\\r\\nCaused by: java.lang.ClassCastException: class org.apache.plc4x.java.opcua.readwrite.ServiceFault cannot be cast to class org.apache.plc4x.java.opcua.readwrite.ReadResponse (org.apache.plc4x.java.opcua.readwrite.ServiceFault and org.apache.plc4x.java.opcua.readwrite.ReadResponse are in unnamed module of loader 'app')\\r\\n        at org.apache.plc4x.java.opcua.protocol.OpcuaProtocolLogic.lambda$read$0(OpcuaProtocolLogic.java:177)\\r\\n        at org.apache.plc4x.java.opcua.context.SecureChannel.lambda$4(SecureChannel.java:212)\\r\\n        at org.apache.plc4x.java.spi.Plc4xNettyWrapper.decode(Plc4xNettyWrapper.java:175)\\r\\n        at io.netty.handler.codec.MessageToMessageCodec$2.decode(MessageToMessageCodec.java:81)\\r\\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:88)\\r\\n        ... 23 more\\r\\n{code}\", 'summary': 'Class casts in new opcua driver'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Martin Z.', 'reporter': 'Martin Z.', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-11-02T00:48:26.000+0000', 'updated': '2022-11-09T10:20:58.000+0000', 'description': 'When using V0.9.0 in PLC4x NIFI-Processor for Modbus TCP, the Processor Task gets stuck, if the network connection is broken. The default timeout and the request-timeout option do not seem to have any affect on this behaviour.\\r\\n\\r\\nIf the NIFI-Processor is started, when the network connection is already broken, a timeout error is thrown. If the connection breaks after a successful initial connection could be made, the running Task gets stuck on the next Modbus TCP Request and has to be forced to terminate.\\r\\n\\r\\nUsing V0.8.0 is working fine in the same scenario with same config in NIFI. In Production, V0.9.0 with Modbus TCP was not usable, since connections broke every day. We had to revert to V0.8.0. This was tested with the PLC4x Build from [https://search.maven.org/search?q=plc4j-nifi-plc4x-nar]\\xa0and the official latest NIFI (V1.14.0) Docker and Windows Version\\xa0', 'summary': 'Modbus TCP Timeout not working. NIFI Processor Task gets stuck'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Fernando ', 'reporter': 'Fernando ', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-11-02T17:36:50.000+0000', 'updated': '2022-11-09T10:21:18.000+0000', 'description': 'Hello, \\r\\n\\r\\nI have been testing the connector and Kafka connects for a few weeks now. To do these tests, I try to ingest measurements from a sensor using Modbus, and this measurement ingestion is done correctly. The problem arises when I decide to remove the connector, as this connection is not closed by default with my sensor, which leads me to serious problems (due to the limitation of devices that read by Modbus, limited to 4 users per manufacture). The only way I can close these connections is to restart the kafka connect container. Is there any option or way to force these connections to close? Additionally, here are the steps to replicate this error: \\r\\n\\r\\n1. Initially, I don\\'t have any connector launched, so I don\\'t have any active connection: active: 0, waiting: 0\\r\\n\\r\\n2. Then, I launch the connector using Kafka Rest API. At this moment, we\\'re importing data into the kafka cluster. Using curl, we can see that the connector is working: curl -X \\'GET\\' http://localhost:18083/connectors/ -> [\"modbus-office\"]. We can see right now that we only have 1 active connection: active: 1, waiting: 0\\r\\n\\r\\n3. I delete the kafka connector using the rest API: curl -X \\'DELETE\\' http://localhost:18083/connectors/modbus-office. So, now, using the previous command from the previous point, we don\\'t see any active connector, but, analysing the active connections from the sensor, it can be seen that there is one active connection: active: 1, waiting: 0. \\r\\n\\r\\nThis is what is giving me problems, as in theory, there should not be any active connections at the moment. We have made a proxy to limit the number of active connections, but so far, we have not been able to close it manually using the REST API of Kafka or some configuration of the connector. \\r\\n\\r\\n\\r\\n', 'summary': 'Modbus - Kafka does not close the connections'}\n",
      "-------------------\n",
      "{'boardId': 365, 'isBacklog': True, 'fixVersion': None, 'priority': 'Blocker', 'assignee': None, 'status': 'Reopened', 'status_category': None, 'creator': 'Otto Fowler', 'reporter': 'Otto Fowler', 'issuetype': 'Bug', 'project': 'Apache PLC4X', 'created': '2021-12-15T17:41:22.000+0000', 'updated': '2022-11-09T10:35:56.000+0000', 'description': '\\r\\n{code:c++}\\r\\n  \\r\\n#ifdef DEBUG_PLC4C_SYSTEM\\r\\n#include <stdio.h>\\r\\n  printf(\"\\\\n~~~~~~~~ PLC4C Connection ~~~~~~~~\\\\n\"\\r\\n    \"Connection String:\\\\t%s\\\\n\"\\r\\n    \"Protocol Code:\\\\t\\\\t%s\\\\n\"\\r\\n    \"Transport Code:\\\\t\\\\t%s\\\\n\"\\r\\n    \"Connection Info:\\\\t%s\\\\n\"\\r\\n    \"Parameters:\\\\t\\\\t%s\\\\n\",\\r\\n    new_connection->connection_string ? new_connection->connection_string : \"NULL\",\\r\\n    new_connection->protocol_code ? new_connection->protocol_code : \"NULL\",\\r\\n    new_connection->transport_code ? new_connection->transport_code : \"NULL\",\\r\\n    new_connection->transport_connect_information ? new_connection->transport_connect_information : \"NULL\",\\r\\n    new_connection->parameters ? new_connection->parameters : \"NULL\");\\r\\n#endif\\r\\n\\r\\n{code}\\r\\n\\r\\n\\r\\nThis code causes an error because of the inclusion of stdio.h in the middle of the unit:\\r\\n\\r\\n{code}\\r\\n2 warnings generated.\\r\\n[  9%] Building C object spi/CMakeFiles/plc4c-spi.dir/src/subscribe.c.o\\r\\n[ 10%] Building C object spi/CMakeFiles/plc4c-spi.dir/src/system.c.o\\r\\nIn file included from /Users/ottofowler/tmp/downloaded-plc4x-0.9.1rc2/0.9.1/rc2/apache-plc4x-0.9.1/plc4c/spi/src/system.c:298:\\r\\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX11.1.sdk/usr/include/stdio.h:260:54: error: function definition is not allowed here\\r\\n__header_always_inline int __sputc(int _c, FILE *_p) {\\r\\n                                                     ^\\r\\n1 error generated.\\r\\nmake[2]: *** [spi/CMakeFiles/plc4c-spi.dir/src/system.c.o] Error 1\\r\\nmake[1]: *** [spi/CMakeFiles/plc4c-spi.dir/all] Error 2\\r\\nmake: *** [all] Error 2\\r\\n{code}\\r\\n\\r\\n\\r\\nSection 7.1.2 Standard headers of the C standard says, in part:\\r\\n\\r\\nIf used, a header shall be included outside of any external declaration or definition, and it shall first be included before the first reference to any of the functions or objects it declares, or to any of the types or macros it defines.\\r\\n\\r\\nSo this isn\\'t really right, I certainly don\\'t think it is good practice in C.  \\r\\n\\r\\nWe should hav two ifdef\\'s one in the header section and one where the code is\\r\\n\\r\\n', 'summary': \"with-c doesn't build on macOS\"}\n",
      "-------------------\n",
      "Processing board: Apache Airavata (ID: 75)\n",
      "Found 0 sprints\n",
      "Found 50 backlog issues\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Sudhakar Pamidighantam', 'reporter': 'Sudhakar Pamidighantam', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2020-03-25T15:24:33.000+0000', 'updated': '2020-03-25T15:24:35.000+0000', 'description': 'It is useful to Categorize Application in a gateway under different heading when 1. there are lot of applications like in SEAGrid.org or if application fall under different disciplines in a multidisciplinary gateway such as DELTA where applications could be categorized as topological analyses, Network Analysis or PCA etc..\\xa0\\r\\n\\r\\n\\xa0', 'summary': 'Categorize Application under flexible Headings in the Dashboard'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Sudhakar Pamidighantam', 'reporter': 'Sudhakar Pamidighantam', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2020-06-05T15:52:31.000+0000', 'updated': '2020-06-05T15:52:32.000+0000', 'description': \"The Globus based sharing mechanism requires following Globus protocols and client and services. In gateways such as DELTA the paritipating groups run simulations outside the gateway and need to transfer trajectory data (100s of GB) to XSEDE resources such as stampede2. They already are familiar with Globus file transfer but the gateway needs specific integration to achieve the shared end point creation and integration through the gateway.This needs to be integrated with Airavata MFT framework eventually.\\r\\n\\r\\n\\xa0\\r\\n|\\xa0|\\r\\n[Ticket #348102: dtuser globus access on stampede2|https://support.globus.org/hc/requests/348102]|\\xa0|\\r\\n|\\xa0|\\xa0\\r\\n[http://support.globus.org/hc/requests/348102]\\r\\n\\r\\n*Lee Liming, Jun 2, 2020, 5:28:08 PM CDT:*\\r\\nHello Sudhakar,\\r\\n\\r\\nWorking with XSEDE personnel, we've received new information, both regarding why the method you're trying to follow isn't working and the recommended alternate method. The document linked below is being prepared by Globus and XSEDE (I'm an author on it, but I'm mainly relating what I've been told) to clarify how science gateways should be set up to interact with Globus services and XSEDE Globus endpoints.\\xa0 It has details on the recommended way for science gateways to work within XSEDE's security policy. \\r\\n\\r\\nHere's the document link:\\r\\n[https://docs.google.com/document/d/1ddt2etkvKV1xGJ6s3_OwHjCCU2-F4o6WYyKNa9fl_N4/edit#|https://docs.google.com/document/d/1ddt2etkvKV1xGJ6s3_OwHjCCU2-F4o6WYyKNa9fl_N4/edit]\\r\\n\\r\\nPlease let us know how the instructions provided here work for you. We (both Globus and XSEDE) would like to share this information more broadly and would appreciate any feedback.\\r\\n\\r\\nNOTE: Toward the end of this document, there's a link to a companion document on how to use sharing on an endpoint to avoid the endpoint activation hassle described here. At this time, the only XSEDE endpoint that supports sharing (as far as we know) is SDSC's Comet. So unfortunately, you won't be able to use that method on Stampede2 or Ranch.\\r\\n\\r\\nThank you,\\r\\n\\r\\nLee Liming|\", 'summary': 'Globus sharing/access to community login space for the gateway'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'In Progress', 'status_category': None, 'creator': 'Eldho Mathulla', 'reporter': 'Eldho Mathulla', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2018-04-26T18:19:56.000+0000', 'updated': '2018-04-26T18:24:38.000+0000', 'description': None, 'summary': 'Implement a cache system for the api server'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'In Progress', 'status_category': None, 'creator': 'Eldho Mathulla', 'reporter': 'Eldho Mathulla', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2018-04-26T18:22:41.000+0000', 'updated': '2018-04-26T18:42:54.000+0000', 'description': 'Design cache interface for the java thrift server', 'summary': 'Design cache interface for the the java thrift servers'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'In Progress', 'status_category': None, 'creator': 'Eldho Mathulla', 'reporter': 'Eldho Mathulla', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2018-04-26T18:24:25.000+0000', 'updated': '2018-04-30T00:34:21.000+0000', 'description': 'Implement the cache api designed for java thrift servers utilizing Redis and Memcached', 'summary': 'Implement Redis and Memecached implementations of Cache API for the java thrift srevers'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Kumar Satyam', 'reporter': 'Kumar Satyam', 'issuetype': 'Improvement', 'project': 'Airavata', 'created': '2017-10-04T18:50:56.000+0000', 'updated': '2017-10-04T18:50:59.000+0000', 'description': 'Run all Airavata containers (PGA+Core+dependencies) on kubernetes/swarm.', 'summary': 'Run all Airavata containers (PGA+Core+dependencies) on kubernetes/swarm.'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Minor', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Suresh Marru', 'reporter': 'Suresh Marru', 'issuetype': 'Improvement', 'project': 'Airavata', 'created': '2011-08-02T21:55:33.000+0000', 'updated': '2013-06-29T12:28:55.000+0000', 'description': 'Workflow Tracking library is used to define the content of the messages to track the progress of the workflow, to track the provenance of the data products generated from the workflow, and to gather computation time and network latency statistics. The schema is now 6 years old and has to be revisited. The namespaces have to be updated to airavata and a maven build profile has to be generate the beans. ', 'summary': 'Revisit workflow tracking schema and library'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Minor', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Suresh Marru', 'reporter': 'Suresh Marru', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2011-08-02T21:57:23.000+0000', 'updated': '2013-06-29T12:29:13.000+0000', 'description': 'Provide detailed documentation on describing and using the workflow tracking schema on the website. ', 'summary': 'Documentation on use of workflow tracking schema'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Suresh Marru', 'reporter': 'Suresh Marru', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2012-11-22T14:57:52.000+0000', 'updated': '2012-11-22T14:57:52.000+0000', 'description': 'Create a test client to submit jobs to MOAB workload manager. To start with, the client can be a sandbox project which we can share with MOAB development team for debugging. ', 'summary': 'Test Client to explore submitting jobs to MOAB '}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Minor', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Chris A. Mattmann', 'reporter': 'Chris A. Mattmann', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2011-07-31T16:08:26.000+0000', 'updated': '2013-08-24T03:53:50.000+0000', 'description': 'Create an OODT File Manager extension point for Airavata for data and workflow cataloging. Also investigate the use of cas-catalog from OODT too. Airavata GFac has an data cataloging interface which by default publishes to the Airavata registry. An implementation to use the rich data cataloging features of OODT will enhance the users to store and retrieve richer metadata from workflow execution.', 'summary': '[GSoC] Create OODT File Manager extension for Airavata'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Suresh Marru', 'reporter': 'Suresh Marru', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2012-03-12T20:08:17.000+0000', 'updated': '2013-08-24T04:06:22.000+0000', 'description': 'Airavata workflow interpreter provides features of provenance aware workflow execution. In a over simplified case If a workflow has 10 components, and if 5th component output is already in there in the registry, then workflow dynamically adapts the graphs and jumps to executes from 6th component. This feature needs the output matching to be semantically correct. Rich metdata expression about outputs and an efficient data cataloging and querying will enhance this capability significantly. \\n\\nThe proposed new feature is to utilize OODT to register, query and adapt the workflow execution based on the enhanced metadata extraction and cataloging. Airavata & OODT developer communities will be able guide in implementing this task.', 'summary': '[GSoC] Integrate Airavata Workflow Interpreter with OODT to improvise provenance aware workflow execution'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Minor', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Mawathage Sanjaya Priyadarshana Medonsa', 'reporter': 'Mawathage Sanjaya Priyadarshana Medonsa', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2013-08-24T04:12:28.000+0000', 'updated': '2013-08-24T04:12:28.000+0000', 'description': 'Initial Apche Airavata-OODT testing is based on the /bin/ls utility. Metadata extractor application based on Apache Tika is an much better application to test the Airavata-OODT integration. Such a application can be considered as a better sample of a scientific application compared /bin/ls utility. ', 'summary': 'Application to extract Metadata from a file using Apache Tika to test OODT-Airavata integration'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Chathura Herath', 'reporter': 'Chathura Herath', 'issuetype': 'Improvement', 'project': 'Airavata', 'created': '2011-12-26T16:43:32.000+0000', 'updated': '2014-07-30T22:26:08.000+0000', 'description': '\\nHeshan has done initial work on how to do parametric sweeps with oversubscription in the nodes for maximum utilization as well as load balancing. This need to be integration with the cartisian product and dot product for-each evaluation of the engine.\\n\\n', 'summary': 'Integrate launched mechanim for foreach- parallel job submissions with the EC2 with oversubscribing'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2012-03-14T16:06:18.000+0000', 'updated': '2016-04-11T15:18:07.000+0000', 'description': \"It is common to need to test something you've created on a test environment rather than on a real setup. This is because,\\n\\n1. The tasks are trivial and it'll be faster to run locally to verify correct flow/execution.\\n2. It can cost to run them on a real setup at development stage.\\n3. Proper access to the real setup is not available\\n4. Unit testing on a real setup is unfeasible\\n\\nThus it would be easy for the user if there is a way to setup a simulated environment without much difficulty may be perhaps with some dummy data from the user itself. Effectively it will be behaving exactly as a GFac server/message box/registry etc but with hard wired data. As an example ideally it would be like the Azure simulator I guess.\\n\\n\", 'summary': '[GSoC] Framework to run/monitor workflows locally with simulated/dummy BE'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Minor', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2012-03-14T15:54:17.000+0000', 'updated': '2016-04-11T15:18:25.000+0000', 'description': \"For a person who works alot with workflows & job executions, would like to have some tools to do common tasks like submitting jobs/workflows, monitoring & analyzing. If he/she can use such a tool with very less effort on configuring & taking less resources it will be quite useful. \\n\\nWhat I'm suggesting is having some simple tools built as plugins for frequently used applications such as web browsers and IDEs. These plugins will give the ability for the user to do atleast the basic workflow execution tasks, monitoring their progress, get notifications , view statistics etc. \", 'summary': '[GSoc] Plugins for running & monitoring workflows'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Minor', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2012-03-13T20:57:27.000+0000', 'updated': '2014-04-01T18:44:36.000+0000', 'description': 'The backend of Airavata should support workflow debugging. It should expose some API that will enable users to connect & \\n1. Receive workflow execution data, current state data\\n2. Send commands to manipulate execution life cycle (pause/resume/restart/stop etc.)\\n3. Modify workflow data on the fly\\n\\nThe API should be intuitive, language independent & supports remote debugging.\\n\\nThe specifics for the task & the level of debugging is open for discussion.', 'summary': '[GSoC] Workflow Debugging Framework for Airavata'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Minor', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Marlon Pierce', 'reporter': 'Marlon Pierce', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2012-03-12T14:44:32.000+0000', 'updated': '2016-04-11T15:19:03.000+0000', 'description': \"Airavata's Registry is used to store and retrieve services and workflows. The Registry is currently very lightweight with an implementation based on Apache Jackrabbit. This task is to develop a more powerful registry capable of handling a wider variety of use cases and science gateway metadata.  Issues include development of JSON metadata models for different use cases, development and evaluation of metadata search capabilities (both expressiveness and performance), and development and evaluation of access policies for the metadata.  The latter may involve integration with Apache Rave as an OpenSocial engine for defining groups and filtering requests.  \\n\\nEvaluating the capabilities of multiple NoSQL databases will be an important component of this task. Candidate technologies from Apache include Cassandra and CouchDB, but appropriately licensed non-Apache projects such as MongoDB will also be evaluated. \\n\\nSample use case scenarios will include computational chemistry, material science, astronomy, and earth science.\\n \", 'summary': '[GSOC] NoSQL implementation for Airavata Registry'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Minor', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Suresh Marru', 'reporter': 'Suresh Marru', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2012-03-11T13:54:21.000+0000', 'updated': '2015-04-09T20:08:17.000+0000', 'description': \"Airavata's GFac enables users to wrap as web services and then use XBaya to drag and drop those components to a workspace and define data flow and control flow dependencies among the application nodes. Airavata's workflow system is used for composing, executing, and monitoring workflow graphs of web service components. The workflow description is high level abstraction and is converted to lower level execution run times like BPEL, SCUFL and Python scripts. \\n\\nAmazon Simple Workflow Service (Amazon SWF) is a newly launched workflow service from Amazon. This service is of interest to Airavata user community to execute coupled applications on Amazon cloud computational resources. \\n\\nExtend Airavata workflow execution to support Amazon SWF will enhance the capabilities provided by Airavata. The airavata developer community will provide detailed guidance and assistance. \\n\\nThere is extensive literature about Airavata WS Messenger including 4 research papers. The Airavata community will provide guidance and assistance with the project. \\n\\nUser community & Impact of the software: Airavata is primarily targeted to build science gateways using computational resources from various disciplines. The initial targeted set of gateways include projects supporting research and education in chemistry, life sciences, biophysics, environmental sciences, geosciences astronomy and nuclear physics. The goal of airavata is to enhance productivity of these gateways to utilize cyberinfrastructure of resources (e.g., local lab resources, the Extreme Science and Engineering Discovery Environment (XSEDE), the Open Science Grid (OSG), University Clusters, Academic and Commercial Computational Clouds like FutureGrid & Amazon EC2). By using open community based software components and services like Airavata, gateways will be able to focus on providing additional scientific capabilities and to expanding the number of supported users. The capabilities of these gateways will offer clear benefits to society.\", 'summary': '[GSoC] Airavata Workflow Enactment to support Amazon Simple Workflow Service'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Task', 'project': 'Airavata', 'created': '2012-05-10T14:32:51.000+0000', 'updated': '2016-12-21T22:05:14.000+0000', 'description': 'Airavata is not a large project, but still there are quite alot of components in it for a developer to get familiar with before diving in on its API. This task will address learning the essentials of APIs present in Airavata & some of the programming aspects related to working with Airavata source code.', 'summary': 'Documenting Airavata from a Developers Perspective'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2012-05-10T15:06:59.000+0000', 'updated': '2016-12-21T22:04:06.000+0000', 'description': \"To work with Airavata it exposes a set of web services.\\n\\n  GFacService\\n  EventingService\\n  WorkflowInterpretor\\n  MsgBoxService\\n  NotificationService\\n\\nIn this task try to figureout what each of these services do and whats the importance of each of it. It'll help understanding the big picture. The Airavata architecture[1] will be of some help.\\n\\n1. http://incubator.apache.org/airavata/architecture/overview.html\", 'summary': 'Documenting the Services exposed in Airavata'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '0.18', 'priority': 'Major', 'assignee': None, 'status': 'Reopened', 'status_category': None, 'creator': 'Sudhakar Pamidighantam', 'reporter': 'Sudhakar Pamidighantam', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2012-06-11T16:41:45.000+0000', 'updated': '2017-01-06T19:53:18.000+0000', 'description': 'It will be useful to interact with the output of an individual task in a workflow while the task is executing. Particularly for long running ( hours)  tasks some intermediate status echos will be very useful for the user to be assured of the right progress of the task. In case of unsatisfactory progress the user may want to stop/pause/kill the workflow and resubmit with modified inputs. If there is a way to modify the inputs and the task rereads them at the next cycle(step) during the execution then the workflow can be steered toward the right (desired) direction by the user. ', 'summary': 'ability to interact with outputs from individual tasks while executing'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Raminderjeet Singh', 'reporter': 'Raminderjeet Singh', 'issuetype': 'Improvement', 'project': 'Airavata', 'created': '2012-08-13T17:28:22.000+0000', 'updated': '2014-07-31T13:23:02.000+0000', 'description': 'This information is very important for the user', 'summary': 'Annotate For-Each(triggered threads) and End-For(completed treads) with number of parallel threads'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Heshan Theekshana Suriyaarachchi', 'reporter': 'Heshan Theekshana Suriyaarachchi', 'issuetype': 'Bug', 'project': 'Airavata', 'created': '2013-03-22T23:14:21.000+0000', 'updated': '2018-12-16T18:29:22.000+0000', 'description': 'We need to decide what to do about these issues.', 'summary': 'Add a round robin scheduling algorithm for EC2 Provider'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Suresh Marru', 'reporter': 'Suresh Marru', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2013-03-24T03:24:53.000+0000', 'updated': '2014-07-31T13:56:25.000+0000', 'description': 'Apache Airavata users construct workflows by chaining together set of applications and web services resulting in a graphical representation of workflows. These workflows composed by drag and drop features build a abstract and high lever workflow languages.Currently Airavata XBaya services these needs and is implemented in Java Swing. Similarly XBaya was also implemented in Flex.\\n\\nThis project focuses on developing a web based version of the workflow composition and monitoring interface similar in functionality to XBaya. Currently XBaya WSDL operations and message type definition determines both the number of input/output parameters that the component has and the data type of each parameter. Messages are general XML and can have deeply-nested structures.  However, XBaya treats the child elements of the root of a message as independent parameters. The type of each parameter can be any simple type (string, integer, etc.), array, or a complex type. The potential student can evaluate the use of WSDL and come up with alternatives. \\n\\nThe student for this task has to be prepared to work extensively in java script to build the drag drop interface. WSDL knowledge will be preferred but not mandatory, that can be acquired. \\n\\nUser community & Impact of the software: Airavata is a general purpose distributed systems software. It is used to build science gateways supporting research and education in chemistry, life sciences, biophysics, environmental sciences, geosciences astronomy and nuclear physics. The goal of airavata is to enhance productivity of these gateways to utilize cyberinfrastructure of resources (e.g., local lab resources, the Extreme Science and Engineering Discovery Environment (XSEDE), the Open Science Grid (OSG), University Clusters, Academic and Commercial Computational Clouds like FutureGrid & Amazon EC2). By using open community based software components and services like Airavata, gateways will be able to focus on providing additional scientific capabilities and to expanding the number of supported users. The capabilities of these gateways will offer clear benefits to society.', 'summary': '[GSoC] Web based Workflow Composer for Airavata '}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Sudhakar Pamidighantam', 'reporter': 'Sudhakar Pamidighantam', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2013-05-03T15:58:22.000+0000', 'updated': '2013-08-21T18:55:54.000+0000', 'description': 'It will be useful and sometimes required to know the status of a task in some detail while the workflow has started executing. Currently the messages return the status of the workflow as a whole. This is good but not sufficient to set the expectation for individual task completion. It will be important to return the status of the individual tasks in a workflow in detail particularly if the task takes more than a few seconds. some status keywords such as Staging Inputs-Scheduled in Batch-Executing-Staging Output-Completed-Error in <Staging-In/Runtime/Staging-Out> etc... could be important to consider.', 'summary': 'Status of task in a workflow'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Subho Banerjee', 'reporter': 'Subho Banerjee', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2013-07-22T08:59:06.000+0000', 'updated': '2015-01-09T04:29:34.000+0000', 'description': 'Adding functionality to the WebUI to register applications and create and store workflows in the registry', 'summary': '[GSoC] Workflow Creation interface'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Sachith Withana', 'reporter': 'Sachith Withana', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2013-12-27T04:27:04.000+0000', 'updated': '2015-04-09T18:09:13.000+0000', 'description': 'add test cases to automate the provider testing. \\nex: Gram, EC2, GSISSH', 'summary': 'Add provider Integration Test cases'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Major', 'assignee': None, 'status': 'Reopened', 'status_category': None, 'creator': 'Suresh Marru', 'reporter': 'Suresh Marru', 'issuetype': 'Story', 'project': 'Airavata', 'created': '2014-02-10T13:05:51.000+0000', 'updated': '2015-06-12T12:57:06.000+0000', 'description': 'The Data model should include management of applications and workflows through a application catalog, basic user and group management and containers for data management. The data management  model should include simple metadata, provenance, monitoring data and Error handling. ', 'summary': 'Craft Airavata Data Models 1.0 '}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Major', 'assignee': None, 'status': 'Reopened', 'status_category': None, 'creator': 'Suresh Marru', 'reporter': 'Suresh Marru', 'issuetype': 'New Feature', 'project': 'Airavata', 'created': '2014-02-10T13:18:24.000+0000', 'updated': '2016-07-07T19:29:51.000+0000', 'description': 'The data model should facilitate retrieving of users for a given gateway. Also enable group management by allowing users to be in multiple groups and retrieval of all users in a group.', 'summary': 'Craft data models for user and group management '}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Suresh Marru', 'reporter': 'Suresh Marru', 'issuetype': 'Improvement', 'project': 'Airavata', 'created': '2014-02-27T22:38:21.000+0000', 'updated': '2015-03-10T15:12:11.000+0000', 'description': 'As we start integrating 0.12, lets gather all the feedback and fixes to do on this JIRA.', 'summary': 'Feedback on Airavata 0.12 Data Models, API'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Raminderjeet Singh', 'reporter': 'Raminderjeet Singh', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-02-27T19:55:21.000+0000', 'updated': '2015-04-20T15:13:54.000+0000', 'description': 'Taking an example of workflow for-each or other parameter sweep cases, DataObjectType.value can be a array object. We can address it later if we want to but when i was mapping these input types to GFAC input types this came up. \\n\\nIf a user has an array of inputs and requires to run the same application for each entry in the input array the workflow data type needs to  support it. Currently an array of inputs is not supported in Airavata workflow.', 'summary': 'In model DataObjectType.value can be a array'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Lahiru Gunathilake', 'reporter': 'Lahiru Gunathilake', 'issuetype': 'Improvement', 'project': 'Airavata', 'created': '2014-03-02T19:55:27.000+0000', 'updated': '2014-07-31T13:54:37.000+0000', 'description': 'After moving to new airavata-data-model and new orchestrator we are not currently supporting EC2 job submission or monitoring.\\n\\nSo we want to support EC2 job registering, submission and monitoring with Airavata.\\n\\nAfter the successful project users should be able to use the airavata-client and register an EC2 job with registry and submit a project and monitor the status. Project owner should write a working sample with above functionality.\\n\\n', 'summary': '[GSOC] Porting EC2 support in new airavata'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Sachith Withana', 'reporter': 'Sachith Withana', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-03-31T01:21:08.000+0000', 'updated': '2015-08-03T04:04:01.000+0000', 'description': None, 'summary': 'Add an Integration test to check Airavata server startup and graceful shutdown'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Trivial', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Story', 'project': 'Airavata', 'created': '2014-03-31T17:06:59.000+0000', 'updated': '2015-05-27T19:03:50.000+0000', 'description': 'Creating integration tests based on real use cases\\n\\nThis JIRA will handle testing all the thrift client API functions relating to usecase of handling the experiment lifecycle equivalent to CIPRES usecase of handling \"Run Task\" gateway user request. ', 'summary': 'Integration Test 2 - Manage Launched Experiments'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Trivial', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-03-31T17:11:48.000+0000', 'updated': '2016-07-13T14:40:27.000+0000', 'description': 'List set of experiment which belongs to the gateway user. Should be able to list Experiment by its state (Unlaunched/Launched/Completed/etc.)', 'summary': 'Integration Test 2 - List Experiments'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Trivial', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-03-31T17:29:25.000+0000', 'updated': '2016-07-13T14:40:37.000+0000', 'description': 'Launch an unlaunched experiment in Airavata', 'summary': 'Integration Test 2 - Launch Experiment'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Trivial', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-03-31T17:32:56.000+0000', 'updated': '2016-07-13T14:40:46.000+0000', 'description': 'Monitor experiment life cycle progress and experiment metadata.', 'summary': 'Integration Test 2 - Monitor Experiment'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': 'WISHLIST', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-03-31T17:43:07.000+0000', 'updated': '2015-06-11T18:25:56.000+0000', 'description': 'Should be able to list what intermediate results are available through the current execution of the experiment. This would include file/directories created in the working directory of the remote job in the remote host, stdout and stderr content and possibly metadata relating to intermediate results (eg: size and/or modified time of a file).\\n\\n(This wont include the workflow execution scenario)', 'summary': 'Integration Test 2 - List Experiment Intermediate Results'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Trivial', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-03-31T17:45:52.000+0000', 'updated': '2016-07-13T14:41:11.000+0000', 'description': 'Retrieve intermediate results of an experiment.', 'summary': 'Integration Test 2 - Download Experiment Intermediate Results'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-03-31T17:49:35.000+0000', 'updated': '2016-07-13T14:54:03.000+0000', 'description': 'Register for status changes of an experiment.', 'summary': 'Integration Test 2 - Notification when Experiment Completes/Fails'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Trivial', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-03-31T17:52:54.000+0000', 'updated': '2016-07-13T14:54:15.000+0000', 'description': 'Could be as same as AIRAVATA-1097 if we are keeping a status for results being downloaded to a per-determined location and we introduce another value to the status enum of the experiment.', 'summary': 'Integration Test 2 - Notifications when Results available for Download'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-03-31T17:55:40.000+0000', 'updated': '2016-07-13T14:54:23.000+0000', 'description': 'Stream the downloaded results. This could be to the portal server or any other machine.', 'summary': 'Integration Test 2 - Download Experiment Final Results'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '1.0', 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Saminda', 'reporter': 'Saminda', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-03-31T17:58:25.000+0000', 'updated': '2016-07-13T14:54:36.000+0000', 'description': 'Incase if the experiments end up in error, the gateway should be able to query the errors relating to the experiment. The errors should be comprehensive enough for the gateway user or gateway admin or gateway developer to take manual steps to fix it or program against the error to resolve or send relevant notifications.', 'summary': 'Integration Test 2 - View Experiment Errors'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Marlon Pierce', 'reporter': 'Marlon Pierce', 'issuetype': 'Story', 'project': 'Airavata', 'created': '2014-04-09T19:42:12.000+0000', 'updated': '2014-04-09T19:42:12.000+0000', 'description': 'Airavata modules with more than one subdirectory need READMEs to describe the code layout.  ', 'summary': 'Modules need READMEs'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': '0.12', 'priority': 'Major', 'assignee': None, 'status': 'Reopened', 'status_category': None, 'creator': 'Marlon Pierce', 'reporter': 'Marlon Pierce', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-04-09T19:42:50.000+0000', 'updated': '2014-05-19T19:54:35.000+0000', 'description': None, 'summary': 'GFAC needs a README'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Marlon Pierce', 'reporter': 'Marlon Pierce', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-04-09T19:43:05.000+0000', 'updated': '2014-05-19T19:54:09.000+0000', 'description': None, 'summary': 'Registry needs a README'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Marlon Pierce', 'reporter': 'Marlon Pierce', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-04-09T19:44:03.000+0000', 'updated': '2014-05-19T19:53:54.000+0000', 'description': None, 'summary': 'Workflow-model needs a README'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Marlon Pierce', 'reporter': 'Marlon Pierce', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-04-09T19:44:26.000+0000', 'updated': '2014-05-19T19:53:40.000+0000', 'description': None, 'summary': 'Commons nees a README'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Marlon Pierce', 'reporter': 'Marlon Pierce', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-04-09T19:45:25.000+0000', 'updated': '2014-05-19T19:53:28.000+0000', 'description': None, 'summary': 'Orchestrator needs a README'}\n",
      "-------------------\n",
      "{'boardId': 75, 'isBacklog': True, 'fixVersion': None, 'priority': 'Major', 'assignee': None, 'status': 'Open', 'status_category': None, 'creator': 'Marlon Pierce', 'reporter': 'Marlon Pierce', 'issuetype': 'Sub-task', 'project': 'Airavata', 'created': '2014-04-09T19:46:06.000+0000', 'updated': '2014-05-19T19:53:17.000+0000', 'description': None, 'summary': 'Tools directory needs a README'}\n",
      "-------------------\n",
      "Processing board: Apache Aurora Twitter Scrum (ID: 37)\n",
      "Found 40 sprints\n",
      "Found 18 issues\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'priority'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     22\u001B[39m dummy_issue[\u001B[33m'\u001B[39m\u001B[33msprintId\u001B[39m\u001B[33m'\u001B[39m] = sprint_id\n\u001B[32m     23\u001B[39m dummy_issue[\u001B[33m'\u001B[39m\u001B[33mkey\u001B[39m\u001B[33m'\u001B[39m] = issue[\u001B[33m'\u001B[39m\u001B[33mkey\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m dummy_issue[\u001B[33m'\u001B[39m\u001B[33mpriority\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43missue\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mpriority\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[33m'\u001B[39m\u001B[33mname\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     25\u001B[39m dummy_issue[\u001B[33m'\u001B[39m\u001B[33mstatus\u001B[39m\u001B[33m'\u001B[39m] = issue[\u001B[33m'\u001B[39m\u001B[33mstatus\u001B[39m\u001B[33m'\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mname\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     26\u001B[39m dummy_issue[\u001B[33m'\u001B[39m\u001B[33mstatus_category\u001B[39m\u001B[33m'\u001B[39m] = issue[\u001B[33m'\u001B[39m\u001B[33mstatus\u001B[39m\u001B[33m'\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mstatusCategory\u001B[39m\u001B[33m'\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mname\u001B[39m\u001B[33m'\u001B[39m]\n",
      "\u001B[31mKeyError\u001B[39m: 'priority'"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T16:44:31.976192Z",
     "start_time": "2025-03-16T16:44:19.507579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save to files\n",
    "with open('apache_boards.json', 'w') as f:\n",
    "    json.dump(all_boards, f)\n",
    "\n",
    "with open('apache_sprints.json', 'w') as f:\n",
    "    json.dump(all_sprints, f)\n",
    "\n",
    "with open('apache_sprint_issues.json', 'w') as f:\n",
    "    json.dump(all_issues, f)\n",
    "\n",
    "with open('apache_backlog_issues.json', 'w') as f:\n",
    "    json.dump(all_backlog_issues, f)"
   ],
   "id": "527420a76ff9ede1",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T16:44:54.312694Z",
     "start_time": "2025-03-16T16:44:50.178715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataFrames for analysis\n",
    "boards_df = pd.json_normalize(all_boards)\n",
    "sprints_df = pd.json_normalize(all_sprints)\n",
    "issues_df = pd.json_normalize(all_issues)\n",
    "backlog_df = pd.json_normalize(all_backlog_issues)"
   ],
   "id": "5e263f77ede7dbd1",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T10:13:40.984849Z",
     "start_time": "2025-03-16T10:13:40.915814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# boards_df = pd.DataFrame(all_boards);\n",
    "# sprints_df = pd.DataFrame(all_sprints);\n",
    "# issues_df = pd.DataFrame(all_issues);\n",
    "# backlog_df = pd.DataFrame(all_backlog_issues);"
   ],
   "id": "422a9fe27dbf1f1a",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T16:45:20.920056Z",
     "start_time": "2025-03-16T16:45:06.351867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save as CSV\n",
    "boards_df.to_csv('./data/apache_boards.csv', index=False)\n",
    "sprints_df.to_csv('./data/apache_sprints.csv', index=False)\n",
    "issues_df.to_csv('./data/apache_sprint_issues.csv', index=False)\n",
    "backlog_df.to_csv('./data/apache_backlog_issues.csv', index=False)"
   ],
   "id": "648916d38086e7b2",
   "outputs": [],
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
